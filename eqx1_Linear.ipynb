{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1st Equinox tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tristantorchet/Desktop/Code/VSCode/LearningJAX/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will follow Equinox official tutorial. It will go in depth and most likely will link to Python's core functionnalities (e.g dataclasses, abstract classes, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Layer and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Linear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, key):\n",
    "        wkey, bkey = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(wkey, (out_size, in_size))\n",
    "        self.bias = jax.random.normal(bkey, (out_size,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "@jax.grad\n",
    "def loss_fn(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)\n",
    "\n",
    "batch_size, in_size, out_size = 32, 2, 3\n",
    "model = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\n",
    "x = jax.numpy.zeros((batch_size, in_size))\n",
    "y = jax.numpy.zeros((batch_size, out_size))\n",
    "grads = loss_fn(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(weight=f32[3,2], bias=f32[3])\n"
     ]
    }
   ],
   "source": [
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Pytrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. JAX core: PyTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, pytree containers can be lists, tuples, dicts, namedtuple, None, OrderedDict. Other types of values, including numeric and ndarray values, are treated as leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structured=CustomLayer(weight=Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), bias=Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32), name='layer1')\n",
      "  flat=[CustomLayer(weight=Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), bias=Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32), name='layer1')]\n",
      "  tree=PyTreeDef(*)\n",
      "  unflattened=CustomLayer(weight=Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), bias=Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32), name='layer1')\n",
      "\n",
      "\n",
      "structured=PyTreeCustomLayer(weight=Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), bias=Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32), name='layer1')\n",
      "  flat=[Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32)]\n",
      "  tree=PyTreeDef(CustomNode(PyTreeCustomLayer[('layer1',)], [*, *]))\n",
      "  unflattened=PyTreeCustomLayer(weight=Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), bias=Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32), name='layer1')\n"
     ]
    }
   ],
   "source": [
    "# create a dummy model as a ABC class with dataclass\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# AbstractLayer is an abstract class which cannot be instantiated\n",
    "# ABC classes with abstract methods insure that all subclasses implement the abstract methods\n",
    "# (It doesn't make sense to use the @dataclass decorator with an ABC class)\n",
    "class AbstractLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, x):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class CustomLayer(AbstractLayer):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "    name: str\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias\n",
    "\n",
    "@jax.tree_util.register_pytree_node_class\n",
    "@dataclass\n",
    "class PyTreeCustomLayer(AbstractLayer):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "    name: str\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        children = (self.weight, self.bias) # the children of the current node\n",
    "        aux_data = (self.name,) # auxiliary data that are not part of the tree structure\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children, *aux_data)\n",
    "\n",
    "\n",
    "w = jax.random.normal(jax.random.PRNGKey(0), (3, 2))\n",
    "b = jax.random.normal(jax.random.PRNGKey(1), (3,))\n",
    "\n",
    "custom_layer = CustomLayer(w, b, 'layer1')\n",
    "pytree_custom_layer = PyTreeCustomLayer(w, b, 'layer1')\n",
    "show_example(custom_layer)\n",
    "print('\\n')\n",
    "show_example(pytree_custom_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32))\n",
      "('layer1',)\n",
      "PyTreeCustomLayer(weight=Array([[ 0.18784384, -1.2833426 ],\n",
      "       [ 0.6494181 ,  1.2490594 ],\n",
      "       [ 0.24447003, -0.11744965]], dtype=float32), bias=Array([ 0.17269018, -0.64765567,  1.2229712 ], dtype=float32), name='layer1')\n"
     ]
    }
   ],
   "source": [
    "leaves, aux_data = pytree_custom_layer.tree_flatten()\n",
    "print(leaves)\n",
    "print(aux_data)\n",
    "\n",
    "layer_reconstructed = PyTreeCustomLayer.tree_unflatten(aux_data, leaves) # will work\n",
    "print(layer_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python objects: https://docs.python.org/3/reference/datamodel.html#basic-customization, https://stackoverflow.com/questions/73409385/object-class-documentation-in-python, https://docs.python.org/3/library/functions.html#object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. All equinox modules ```eqx.Modules``` are Pytrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Linear'>\n",
      "Linear(weight=f32[3,2], bias=f32[3])\n",
      "[[0.59902626 0.2172144 ]\n",
      " [0.660603   0.03266738]\n",
      " [1.2164948  1.1940813 ]]\n",
      "[ 1.1378784  -1.2209548  -0.59153634]\n"
     ]
    }
   ],
   "source": [
    "batch_size, in_size, out_size = 32, 2, 3\n",
    "model = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\n",
    "\n",
    "print(type(model))\n",
    "print(model)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Linear'>\n",
      "(<class 'equinox._module.Module'>,)\n",
      "(<class 'object'>,)\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(model).__bases__)\n",
    "print(type(model).__bases__[0].__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([[0.59902626, 0.2172144 ],\n",
      "       [0.660603  , 0.03266738],\n",
      "       [1.2164948 , 1.1940813 ]], dtype=float32), Array([ 1.1378784 , -1.2209548 , -0.59153634], dtype=float32)] \n",
      "\n",
      "PyTreeDef(CustomNode(Linear[('weight', 'bias'), (), ()], [*, *]))\n"
     ]
    }
   ],
   "source": [
    "leaves, aux_data = jax.tree_util.tree_flatten(model)\n",
    "print(leaves, '\\n')\n",
    "print(aux_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 2), (3,)]\n",
      "Linear(weight=(3, 2), bias=(3,))\n"
     ]
    }
   ],
   "source": [
    "tree_mapped_fn = lambda x: x.shape \n",
    "print(jax.tree_util.tree_map(tree_mapped_fn, jax.tree_util.tree_flatten(model, is_leaf=lambda x: isinstance(x, jax.Array))[0]))\n",
    "print(jax.tree_util.tree_map(tree_mapped_fn, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (2,) and (32,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batched_input \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m), (batch_size, in_size))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/QSSM/.venv/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:264\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    262\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/QSSM/.venv/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:3413\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   3411\u001b[0m a \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39msqueeze(a, \u001b[38;5;28mtuple\u001b[39m(a_squeeze))\n\u001b[1;32m   3412\u001b[0m b \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39msqueeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[0;32m-> 3413\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3414\u001b[0m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3415\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3416\u001b[0m result \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mtranspose(out, perm)\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/QSSM/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:2610\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   2608\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2609\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2610\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (2,) and (32,)."
     ]
    }
   ],
   "source": [
    "batched_input = jax.random.normal(jax.random.PRNGKey(0), (batch_size, in_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: dot_general requires contracting dimensions to have the same shape, got (2,) and (32,).\n",
      "w @ x = (3, 2), (32, 2)\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    print(model(batched_input))\n",
    "except Exception as e:\n",
    "    print(f'ERROR: {e}')\n",
    "    print(f'w @ x = {model.weight.shape}, {batched_input.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n",
      "(32, 3)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "out = jax.vmap(model)(batched_input)\n",
    "print(out.shape)\n",
    "out_manual = batched_input @ model.weight.T + model.bias\n",
    "print(out_manual.shape)\n",
    "print(jnp.allclose(out, out_manual, atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmap time: 1.76 ms\n",
      "manual time: 0.50 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "out = jax.vmap(model)(batched_input)\n",
    "end = time.time()\n",
    "print(f'vmap time: {(end - start)*1e3:.2f} ms')\n",
    "\n",
    "start = time.time()\n",
    "out_manual = batched_input @ model.weight.T + model.bias\n",
    "end = time.time()\n",
    "print(f'manual time: {(end - start)*1e3:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tristantorchet/Desktop/Code/VSCode/QSSM/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jaxtyping in /Users/tristantorchet/Desktop/Code/VSCode/QSSM/.venv/lib/python3.12/site-packages (0.2.36)\n"
     ]
    }
   ],
   "source": [
    "!pip install jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "    layer1: eqx.nn.Linear\n",
    "    bn1: eqx.nn.BatchNorm\n",
    "    layer2: eqx.nn.Linear\n",
    "    bn2: eqx.nn.BatchNorm\n",
    "    layer3: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, out_size, key):\n",
    "        self.layer1 = eqx.nn.Linear(in_size, hidden_size, key)\n",
    "        self.bn1 = eqx.nn.BatchNorm(hidden_size, axis_name='batch')\n",
    "        self.layer2 = eqx.nn.Linear(hidden_size, hidden_size, key)\n",
    "        self.bn2 = eqx.nn.BatchNorm(hidden_size, axis_name='batch')\n",
    "        self.layer3 = eqx.nn.Linear(hidden_size, out_size, key)\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        x = self.layer1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is just a weird mish-mash of stateful and non-stateful layers for\n",
    "# demonstration purposes, it isn't doing any clever.\n",
    "class Model(eqx.Module):\n",
    "    norm1: eqx.nn.BatchNorm\n",
    "    spectral_linear: eqx.nn.SpectralNorm[eqx.nn.Linear]\n",
    "    norm2: eqx.nn.BatchNorm\n",
    "    linear1: eqx.nn.Linear\n",
    "    linear2: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jr.split(key, 4)\n",
    "        self.norm1 = eqx.nn.BatchNorm(input_size=3, axis_name=\"batch\")\n",
    "        self.spectral_linear = eqx.nn.SpectralNorm(\n",
    "            layer=eqx.nn.Linear(in_features=3, out_features=32, key=key1),\n",
    "            weight_name=\"weight\",\n",
    "            key=key2,\n",
    "        )\n",
    "        self.norm2 = eqx.nn.BatchNorm(input_size=32, axis_name=\"batch\")\n",
    "        self.linear1 = eqx.nn.Linear(in_features=32, out_features=32, key=key3)\n",
    "        self.linear2 = eqx.nn.Linear(in_features=32, out_features=3, key=key4)\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        x, state = self.norm1(x, state)\n",
    "        x, state = self.spectral_linear(x, state)\n",
    "        x = jax.nn.relu(x)\n",
    "        x, state = self.norm2(x, state)\n",
    "        x = self.linear1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, state, xs, ys):\n",
    "    batch_model = jax.vmap(\n",
    "        model, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n",
    "    )\n",
    "    pred_ys, state = batch_model(xs, state)\n",
    "    loss = jnp.mean((pred_ys - ys) ** 2)\n",
    "    return loss, state\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, state, opt_state, xs, ys):\n",
    "    grads, state = eqx.filter_grad(compute_loss, has_aux=True)(model, state, xs, ys)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(\n",
      "  0x1015c5948=bool[],\n",
      "  0x1015c5968=(f32[3], f32[3]),\n",
      "  0x1015c5988=(f32[32], f32[3]),\n",
      "  0x1015c59a8=bool[],\n",
      "  0x1015c59c8=(f32[32], f32[32])\n",
      ")\n",
      "State(\n",
      "  0x1015c5948=bool[],\n",
      "  0x1015c5968=(f32[3], f32[3]),\n",
      "  0x1015c5988=(f32[32], f32[3]),\n",
      "  0x1015c59a8=bool[],\n",
      "  0x1015c59c8=(f32[32], f32[32])\n",
      ")\n",
      "State(\n",
      "  0x1015c5948=bool[],\n",
      "  0x1015c5968=(f32[3], f32[3]),\n",
      "  0x1015c5988=(f32[32], f32[3]),\n",
      "  0x1015c59a8=bool[],\n",
      "  0x1015c59c8=(f32[32], f32[32])\n",
      ")\n",
      "State(\n",
      "  0x1015c5948=bool[],\n",
      "  0x1015c5968=(f32[3], f32[3]),\n",
      "  0x1015c5988=(f32[32], f32[3]),\n",
      "  0x1015c59a8=bool[],\n",
      "  0x1015c59c8=(f32[32], f32[32])\n",
      ")\n",
      "State(\n",
      "  0x1015c5948=bool[],\n",
      "  0x1015c5968=(f32[3], f32[3]),\n",
      "  0x1015c5988=(f32[32], f32[3]),\n",
      "  0x1015c59a8=bool[],\n",
      "  0x1015c59c8=(f32[32], f32[32])\n",
      ")\n",
      "State(\n",
      "  0x1015c5948=bool[],\n",
      "  0x1015c5968=(f32[3], f32[3]),\n",
      "  0x1015c5988=(f32[32], f32[3]),\n",
      "  0x1015c59a8=bool[],\n",
      "  0x1015c59c8=(f32[32], f32[32])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 10\n",
    "learning_rate = 3e-4\n",
    "steps = 5\n",
    "seed = 5678\n",
    "\n",
    "key = jr.PRNGKey(seed)\n",
    "mkey, xkey, xkey2 = jr.split(key, 3)\n",
    "\n",
    "model, state = eqx.nn.make_with_state(Model)(mkey)\n",
    "print(state)\n",
    "\n",
    "xs = jr.normal(xkey, (dataset_size, 3))\n",
    "ys = jnp.sin(xs) + 1\n",
    "optim = optax.adam(learning_rate)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "for _ in range(steps):\n",
    "    # Full-batch gradient descent in this simple example.\n",
    "    model, state, opt_state = make_step(model, state, opt_state, xs, ys)\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
