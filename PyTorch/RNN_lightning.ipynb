{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Custom Elman RNN for MNIST Classification (row and sequential)\n",
    "\n",
    "In this notebook, we implement a **custom Elman RNN** to classify the **MNIST dataset** in both **row-wise** and **sequential** formats.\n",
    "\n",
    "### Overview of the Implementation:\n",
    "We define two key components:\n",
    "1. ````CustomRNNLayer```` – A single-layer recurrent neural network (Elman RNN).\n",
    "2. ````RNNBackbone```` – A full RNN-based model that stacks one or more `CustomRNNLayer` instances and adds a ```final linear layer``` to classify MNIST digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import lightning as L\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys \n",
    "sys.path.append('../Flax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "NVIDIA GeForce RTX 4090\n",
      "cuda\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_mnist_classification_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating MNIST Classification Dataset...\n",
      "(128, 784, 1)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATASET_VERSION = \"sequential\" # \"sequential\" or \"row\"\n",
    "\n",
    "# Create dataset\n",
    "train_loader, val_loader, test_loader, n_classes, seq_length, in_dim = create_mnist_classification_dataset(\n",
    "    bsz=BATCH_SIZE, version=DATASET_VERSION\n",
    ")\n",
    "\n",
    "# split the dataset into train and validation\n",
    "train_size = int(0.8 * len(train_loader.dataset))\n",
    "valid_size = len(train_loader.dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "    train_loader.dataset, [train_size, valid_size]\n",
    ")\n",
    "\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "print(batch_images.shape)\n",
    "print(batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "class LightningRNNBackbone(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, criterion, batch_size):\n",
    "        '''\n",
    "        RNN backbone using 1 recurrent layer and 1 readout layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "        self.W_out = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.logger_kwargs = {\"batch_size\": batch_size, \"on_epoch\": True, \"on_step\":True, \"prog_bar\": True}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [seq_len, input_size] or [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # state_hist, out_hist = self.rnn_layer(x)\n",
    "        state_hist, _ = self.rnn_layer(x)\n",
    "        out_hist = self.W_out(state_hist)\n",
    "        return state_hist, out_hist\n",
    "    \n",
    "    def run_batch(self, batch):\n",
    "        # x shape: [seq_len, input_size] or [batch_size, seq_len, input_size]\n",
    "        x, y = batch\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(self.device)\n",
    "        # state_hist, out_hist = self.rnn_layer(x)\n",
    "        state_hist, _ = self(x)\n",
    "        out_hist = self.W_out(state_hist)\n",
    "        final_outputs = out_hist[:, -1, :]\n",
    "        return self.criterion(final_outputs, y)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        loss = self.run_batch(batch)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        # self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, batch_size=128)\n",
    "        self.log(\"train_loss\", loss, **self.logger_kwargs) \n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        loss = self.run_batch(batch)\n",
    "        self.log(\"val_loss\", loss, **self.logger_kwargs) \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        loss = self.run_batch(batch)\n",
    "        self.log(\"test_loss\", loss, **self.logger_kwargs) \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = LightningRNNBackbone(in_dim, HIDDEN_SIZE, n_classes, nn.CrossEntropyLoss(), BATCH_SIZE)\n",
    "model.to(device)\n",
    "print(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0      | train\n",
      "1 | rnn_layer | RNN              | 4.3 K  | train\n",
      "2 | W_out     | Linear           | 650    | train\n",
      "-------------------------------------------------------\n",
      "4.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 303.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: |          | 0/? [00:00<?, ?it/s]                                 \n",
      "Epoch 0: 100%|██████████| 200/200 [00:04<00:00, 41.29it/s, v_num=6, train_loss_step=2.310, val_loss_step=2.290, val_loss_epoch=2.300, train_loss_epoch=2.300]\n",
      "Epoch 1: 100%|██████████| 200/200 [00:04<00:00, 40.94it/s, v_num=6, train_loss_step=2.310, val_loss_step=2.290, val_loss_epoch=2.300, train_loss_epoch=2.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from lightning.pytorch.callbacks import RichProgressBar\n",
    "\n",
    "# # train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "# callbacks = [RichProgressBar(leave=True)]\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"logs/\", name=\"rnn_experiment\")\n",
    "\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "callbacks = [TQDMProgressBar(leave=True)]\n",
    "trainer = L.Trainer(max_epochs=2, callbacks=callbacks,\n",
    "                    log_every_n_steps=50,\n",
    "                    limit_train_batches=200, \n",
    "                    enable_progress_bar=True, logger=logger)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "/home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 78/78 [00:01<00:00, 74.18it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch        2.2979655265808105\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 2.2979655265808105}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "trainer.test(model, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ipywidgets 8.1.5\n",
      "Uninstalling ipywidgets-8.1.5:\n",
      "  Successfully uninstalled ipywidgets-8.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall ipywidgets -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from tensorboard) (1.26.3)\n",
      "Requirement already satisfied: packaging in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from tensorboard) (24.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from tensorboard) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, tensorboard\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "orbax-checkpoint 0.5.0 requires etils[epath,epy], which is not installed.\n",
      "orbax-checkpoint 0.5.0 requires tensorstore>=0.1.51, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.70.0 markdown-3.7 protobuf-5.29.3 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_conda_env_LearningJAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
