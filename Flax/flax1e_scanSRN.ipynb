{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tristan/LearningJAX/Flax\n",
      "/home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/bin/python\n",
      "flax==0.8.0\n",
      "jax==0.4.25\n",
      "jaxlib==0.4.25+cuda11.cudnn86\n",
      "optax==0.1.8\n",
      "orbax-checkpoint==0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!which python\n",
    "!pip freeze | grep -E 'flax|jax|orbax|optax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chex==0.1.86\n",
      "  Using cached chex-0.1.86-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: absl-py>=0.9.0 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from chex==0.1.86) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from chex==0.1.86) (4.12.2)\n",
      "Requirement already satisfied: jax>=0.4.16 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from chex==0.1.86) (0.4.25)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from chex==0.1.86) (0.4.25+cuda11.cudnn86)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from chex==0.1.86) (1.26.3)\n",
      "Collecting toolz>=0.9.0 (from chex==0.1.86)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: setuptools in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from chex==0.1.86) (75.8.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from jax>=0.4.16->chex==0.1.86) (0.5.1)\n",
      "Requirement already satisfied: opt-einsum in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from jax>=0.4.16->chex==0.1.86) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages (from jax>=0.4.16->chex==0.1.86) (1.15.2)\n",
      "Using cached chex-0.1.86-py3-none-any.whl (98 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: toolz, chex\n",
      "Successfully installed chex-0.1.86 toolz-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install chex==0.1.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import torch\n",
    "from jax import numpy as jnp\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from flax import linen as nn\n",
    "from jax.nn.initializers import lecun_normal\n",
    "from typing import Any, Tuple, Sequence, Optional\n",
    "\n",
    "jnp.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# set cuda visible devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this code is from QSSM project and won't be updated \n",
    "def create_mnist_classification_dataset(bsz=128, root=\"./data\"):\n",
    "    print(\"[*] Generating MNIST Classification Dataset...\")\n",
    "\n",
    "    # Constants\n",
    "    SEQ_LENGTH, N_CLASSES, IN_DIM = 784, 10, 1\n",
    "    tf = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=0.5, std=0.5),\n",
    "            transforms.Lambda(lambda x: x.view(IN_DIM, SEQ_LENGTH).t()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train = torchvision.datasets.MNIST(\n",
    "        root, train=True, download=True, transform=tf\n",
    "    )\n",
    "    test = torchvision.datasets.MNIST(\n",
    "        root, train=False, download=True, transform=tf\n",
    "    )\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        transposed_data = list(zip(*batch))\n",
    "        labels = np.array(transposed_data[1])\n",
    "        images = np.array(transposed_data[0])\n",
    "\n",
    "        return images, labels       \n",
    "\n",
    "\n",
    "    # Return data loaders, with the provided batch size\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=bsz, shuffle=True, collate_fn=custom_collate_fn\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        test, batch_size=bsz, shuffle=False, collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader, N_CLASSES, SEQ_LENGTH, IN_DIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating MNIST Classification Dataset...\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader, N_CLASSES, SEQ_LENGTH, IN_DIM = create_mnist_classification_dataset(root=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784, 1) (128,)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = next(iter(testloader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "print(batch_y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, state, x):\n",
    "        # Wh @ h + Wx @ x + b can be efficiently computed\n",
    "        # by concatenating the vectors and then having a single dense layer\n",
    "        x = jnp.concatenate([state, x])\n",
    "        new_state = jnp.tanh(nn.Dense(state.shape[0], name='Dense_RNN')(x))\n",
    "        return new_state\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        rnn_cell = RNNCell()\n",
    "        state = jnp.zeros((self.hidden_size,))\n",
    "        dense_out = nn.Dense(self.output_size, name='Dense_Out')\n",
    "        # out_ = []\n",
    "        for t in range(x.shape[0]):\n",
    "            state = rnn_cell(state, x[t])\n",
    "            out = dense_out(state)\n",
    "        #     # if we want to track at each time step, otherwize only the last one is enough\n",
    "        #     out_.append(out)\n",
    "        # out = jnp.stack(out_)\n",
    "        return out\n",
    "    \n",
    "BatchRNN = nn.vmap(RNN, in_axes=0, out_axes=0, variable_axes={'params': 0}, split_rngs={'params': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srn = BatchRNN(256, 10)\n",
    "variables = srn.init(jax.random.PRNGKey(0), jnp.zeros((1,784,1)))\n",
    "y = srn.apply(variables, jnp.zeros((1,784,1)))\n",
    "y.shape # (batch, time, cell_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "\n",
    "# def create_train_state(key, model_cls, lr):\n",
    "#     '''\n",
    "#     Create the training state for the model.\n",
    "#     '''\n",
    "#     model = model_cls(hidden_size=64, output_size=10)\n",
    "#     init_x = jnp.ones((1, 784, 1))  # Example batch of inputs\n",
    "\n",
    "#     params = model.init(key, init_x)['params']\n",
    "#     print(\"Initialized parameter structure:\", jax.tree_util.tree_map(jnp.shape, params))\n",
    "#     # use adam \n",
    "#     optimizer = optax.sgd(learning_rate=lr, momentum=0.9)\n",
    "#     return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "lr = 1e-2\n",
    "# state = create_train_state(subkey, RNN, lr)\n",
    "# print(state.params.keys())\n",
    "# print(state.params['RNNCell_0'])\n",
    "# print(state.params['Dense_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state.params['Dense_0']['kernel'].shape)\n",
    "# print(state.params['Dense_0']['bias'].shape)\n",
    "# print(train_state.params['output']['kernel'].shape)\n",
    "# print(train_state.params['output']['bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(state, images, labels):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, images)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(state, train_dl, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    progress_bar = tqdm(train_dl, desc=\"Training\", leave=True)\n",
    "    batch_id = 0\n",
    "    for batch_images, batch_labels in progress_bar:\n",
    "        grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "        # print(jnp.max(grads['dense_0']['kernel']), jnp.min(grads['dense_0']['kernel']))\n",
    "        # print(loss)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "        batch_id += 1\n",
    "        if batch_id % 3 == 0:\n",
    "            progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n",
    "        \n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(key, model_cls, lr):\n",
    "    init_x = jnp.ones((128, 784, 1))  # Example batch of inputs\n",
    "\n",
    "    model = model_cls(hidden_size=256, output_size=10)\n",
    "    params = model.init(key, init_x)['params']\n",
    "    \n",
    "    # Debugging: Print parameter structure\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(key, BatchRNN, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 6/469 [01:10<1:27:52, 11.39s/it, accuracy=0.0859, loss=3.7]"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    state, train_loss, train_accuracy = run_epoch(state, trainloader, key)\n",
    "    print(f\"Epoch {epoch} | Loss: {train_loss} | Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_conda_env_LearningJAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
