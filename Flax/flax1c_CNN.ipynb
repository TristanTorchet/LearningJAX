{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tristantorchet/Desktop/Code/VSCode/LearningJAX/Flax\n",
      "/Users/tristantorchet/Desktop/Code/VSCode/LearningJAX/.venv/bin/python\n",
      "flax==0.8.0\n",
      "jax==0.4.25\n",
      "jaxlib==0.4.25\n",
      "jaxtyping==0.2.36\n",
      "optax==0.1.8\n",
      "orbax-checkpoint==0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!which python\n",
    "!pip freeze | grep -E 'flax|jax|orbax|optax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import torch\n",
    "from jax import numpy as jnp\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from flax import linen as nn\n",
    "from jax.nn.initializers import lecun_normal\n",
    "from typing import Any, Tuple, Sequence, Optional\n",
    "\n",
    "jnp.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this code is from QSSM project and won't be updated \n",
    "def create_mnist_classification_dataset(bsz=128, root=\"./data\"):\n",
    "    print(\"[*] Generating MNIST Classification Dataset...\")\n",
    "\n",
    "    # Constants\n",
    "    SEQ_LENGTH, N_CLASSES, IN_DIM = 784, 10, 1\n",
    "    tf = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=0.5, std=0.5),\n",
    "            #transforms.Lambda(lambda x: x.view(IN_DIM, SEQ_LENGTH).t()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train = torchvision.datasets.MNIST(\n",
    "        root, train=True, download=True, transform=tf\n",
    "    )\n",
    "    test = torchvision.datasets.MNIST(\n",
    "        root, train=False, download=True, transform=tf\n",
    "    )\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        transposed_data = list(zip(*batch))\n",
    "        labels = np.array(transposed_data[1])\n",
    "        images = np.array(transposed_data[0])\n",
    "\n",
    "        return images, labels       \n",
    "\n",
    "\n",
    "    # Return data loaders, with the provided batch size\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=bsz, shuffle=True, collate_fn=custom_collate_fn\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        test, batch_size=bsz, shuffle=False, collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader, N_CLASSES, SEQ_LENGTH, IN_DIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating MNIST Classification Dataset...\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader, N_CLASSES, SEQ_LENGTH, IN_DIM = create_mnist_classification_dataset(root=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28) (128,)\n",
      "int64\n",
      "(128, 1, 28, 28, 1)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = next(iter(testloader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "print(batch_y.dtype)\n",
    "# convert batch_y to float\n",
    "# add an extra dimension to batch_x\n",
    "batch_x = batch_x[..., None]\n",
    "print(batch_x.shape)\n",
    "print(batch_y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(batch_x[0][0,:,:,0])\n",
    "batch_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "\n",
    "def create_train_state(key, model_cls, lr):\n",
    "    '''\n",
    "    Create the training state for the model.\n",
    "    '''\n",
    "    model = model_cls()\n",
    "    params = model.init(key, jnp.ones((1, 28, 28, 1)))['params']\n",
    "    # use adam \n",
    "    optimizer = optax.sgd(learning_rate=lr, momentum=0.9)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Conv_0', 'Conv_1', 'Dense_0', 'Dense_1'])\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "lr = 1e-2\n",
    "train_state = create_train_state(subkey, CNN, lr)\n",
    "print(train_state.params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3136, 256)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "print(train_state.params['Dense_0']['kernel'].shape)\n",
    "print(train_state.params['Dense_0']['bias'].shape)\n",
    "# print(train_state.params['output']['kernel'].shape)\n",
    "# print(train_state.params['output']['bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(state, images, labels):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, images)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(state, train_dl, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    progress_bar = tqdm(train_dl, desc=\"Training\", leave=True)\n",
    "    batch_id = 0\n",
    "    for batch_images, batch_labels in progress_bar:\n",
    "        grads, loss, accuracy = apply_model(state, batch_images[...,None], batch_labels)\n",
    "        # print(jnp.max(grads['dense_0']['kernel']), jnp.min(grads['dense_0']['kernel']))\n",
    "        # print(loss)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "        batch_id += 1\n",
    "        if batch_id % 100 == 0:\n",
    "            progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n",
    "        \n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.4293811321258545 | Accuracy: 0.8711076378822327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:34<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.08796628564596176 | Accuracy: 0.9734808206558228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.06289806216955185 | Accuracy: 0.9811378717422485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.05145036801695824 | Accuracy: 0.9844304919242859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 50/469 [00:03<00:30, 13.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_state, train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(state, train_dl, rng)\u001b[0m\n\u001b[1;32m      5\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_images, batch_labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[0;32m----> 8\u001b[0m     grads, loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(jnp.max(grads['Dense_0']['kernel']), jnp.min(grads['Dense_0']['kernel']))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     state \u001b[38;5;241m=\u001b[39m update_model(state, grads)\n",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m, in \u001b[0;36mapply_model\u001b[0;34m(state, images, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n\u001b[1;32m     10\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m (loss, logits), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(jnp\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads, loss, accuracy\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/api.py:751\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m _check_scalar(ans)\n\u001b[1;32m    750\u001b[0m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n\u001b[0;32m--> 751\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mvjp_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m g \u001b[38;5;241m=\u001b[39m g[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m g\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/tree_util.py:488\u001b[0m, in \u001b[0;36m_HashableCallableShim.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m--> 488\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/api.py:2163\u001b[0m, in \u001b[0;36m_vjp_pullback_wrapper\u001b[0;34m(name, cotangent_dtypes, cotangent_shapes, io_tree, fun, *py_args_)\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshape(arg) \u001b[38;5;241m!=\u001b[39m ct_shape:\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of cotangent input to vjp pullback function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be the same as the shape of corresponding primal input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2163\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, ans)\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/tree_util.py:488\u001b[0m, in \u001b[0;36m_HashableCallableShim.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m--> 488\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:150\u001b[0m, in \u001b[0;36mvjp.<locals>.unbound_vjp\u001b[0;34m(pvals, jaxpr, consts, *cts)\u001b[0m\n\u001b[1;32m    148\u001b[0m cts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known())\n\u001b[1;32m    149\u001b[0m dummy_args \u001b[38;5;241m=\u001b[39m [UndefinedPrimal(v\u001b[38;5;241m.\u001b[39maval) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m jaxpr\u001b[38;5;241m.\u001b[39minvars]\n\u001b[0;32m--> 150\u001b[0m arg_cts \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(instantiate_zeros, arg_cts)\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:245\u001b[0m, in \u001b[0;36mbackward_pass\u001b[0;34m(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m   cts_in, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(read_cotangent, eqn\u001b[38;5;241m.\u001b[39moutvars)\n\u001b[0;32m--> 245\u001b[0m name_stack \u001b[38;5;241m=\u001b[39m \u001b[43msource_info_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_name_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m eqn\u001b[38;5;241m.\u001b[39msource_info\u001b[38;5;241m.\u001b[39mname_stack\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m source_info_util\u001b[38;5;241m.\u001b[39muser_context(eqn\u001b[38;5;241m.\u001b[39msource_info\u001b[38;5;241m.\u001b[39mtraceback, name_stack\u001b[38;5;241m=\u001b[39mname_stack):\n\u001b[1;32m    247\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m eqn\u001b[38;5;241m.\u001b[39mprimitive\u001b[38;5;241m.\u001b[39mcall_primitive \u001b[38;5;129;01mor\u001b[39;00m eqn\u001b[38;5;241m.\u001b[39mprimitive\u001b[38;5;241m.\u001b[39mmap_primitive:\n",
      "File \u001b[0;32m~/Desktop/Code/VSCode/LearningJAX/.venv/lib/python3.12/site-packages/jax/_src/source_info_util.py:265\u001b[0m, in \u001b[0;36mcurrent_name_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m     _source_info_context\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m prev\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_name_stack\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NameStack:\n\u001b[1;32m    266\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _source_info_context\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mname_stack\n\u001b[1;32m    268\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextend_name_stack\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[NameStack]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    train_state, train_loss, train_accuracy = run_epoch(train_state, trainloader, key)\n",
    "    print(f\"Epoch {epoch} | Loss: {train_loss} | Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
