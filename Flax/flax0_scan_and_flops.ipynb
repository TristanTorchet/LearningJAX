{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook focuses on flax.linen.scan function, key for any recurrent model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I show: \n",
    "- Two ways of implementing a custom initialization for a Flax model.\n",
    "- How to use the `nn.scan` function to implement a simple RNN.\n",
    "- How to use the `tabulate` function to display the model's parameters.\n",
    "- How to compute the flop of this model. \n",
    "- Maximum FLOPs achievable by the GPU through JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement: \n",
    "\n",
    "$\n",
    "h_t = \\tanh(W_{hh} h_{t-1} + (W_{xh} x_t + b_h))\n",
    "$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, $W_{hh}$, $W_{xh}$ and $b_h$ are the weights and bias of the RNN.\n",
    "\n",
    "- $W_{hh} h_{t-1}$ is computed via an explicit matrix multiplication (`jnp.dot`, actually we compute $h_{t-1} W_{hh}$).\n",
    "- $(W_{xh} x_t + b_h)$ is computed via a `nn.Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I go, `nn.scan` can only be used inside a __call__ method on a function also defined inside the __call__ method. \n",
    "\n",
    "Also, the scanned function must use the signature `fn(self, carry, x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (20, 10)\n"
     ]
    }
   ],
   "source": [
    "def custom_w_init():\n",
    "    def init(rng, shape):\n",
    "        return jax.random.uniform(rng, shape, minval=-0.1, maxval=0.1)\n",
    "    return init\n",
    "\n",
    "def another_custom_w_init(rng, shape, dtype=jnp.float32):\n",
    "    return jax.random.uniform(rng, shape, minval=-0.1, maxval=0.1, dtype=dtype)\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    hidden_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        Wh = self.param('W', custom_w_init(), (self.hidden_dim, self.hidden_dim))\n",
    "        dense_in = nn.Dense(features=self.hidden_dim, kernel_init=another_custom_w_init)\n",
    "        h = jnp.zeros((self.hidden_dim,))\n",
    "\n",
    "        def update(self, h, x):\n",
    "            h = jnp.tanh(jnp.dot(h, Wh) + dense_in(x))\n",
    "            return h, h # Return the new carry and the output   \n",
    "        \n",
    "        scan_update = nn.scan(\n",
    "            update,\n",
    "            variable_broadcast='params',\n",
    "            in_axes=0,\n",
    "            out_axes=0\n",
    "        )\n",
    "        \n",
    "        return scan_update(self, h, x)\n",
    "\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "x = jnp.ones((20, 100))  # 5 timesteps, input size 10\n",
    "HIDDEN_DIM = 10\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key)\n",
    "model = RNNCell(HIDDEN_DIM)\n",
    "params = model.init(key, x)\n",
    "out, hist = model.apply(params, x)\n",
    "print(out.shape, hist.shape)  # (10,) (20, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (200000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "HIDDEN_DIM = 100\n",
    "x = jnp.ones((200000, HIDDEN_DIM))  # 5 timesteps, input size 10\n",
    "\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = RNNCell(hidden_dim=HIDDEN_DIM)\n",
    "params = model.init(key, x)\n",
    "out, hist = model.apply(params, x)\n",
    "print(out.shape, hist.shape)  # (5, 10) (5, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute the flops of this RNN\n",
    "- $W_h h_{t-1}$ is a matrix multiplication of size 100x100 -> 100x(100 mults + 99 adds) = 19'900 flops\n",
    "- $W_x x_t$ is a matrix multiplication of size 100x100 -> 100x(100 mults + 99 adds) = 19'900 flops\n",
    "- $W_h h_{t-1} + W_x x_t$ is an addition of two vectors of size 100 -> 100 adds = 100 flops\n",
    "- The activation function is 100 flops --> 100 flop\n",
    "- The total flop per time step is 19'900 + 19'900 + 100 + 100 = 39'000 flop\n",
    "- We do 200'000 time steps --> 200'000 * 39'000 = 7'800'000'000 flop\n",
    "- The runtime is 1.95s --> 7'800'000'000 flop / 1.95s = 4'000'000'000 flop/s = 4 GFLOP/s\n",
    "- The jit runtime is 1.85s --> 7'800'000'000 flop / 1.85s = 4'216'216'216 flop/s = 4.2 GFLOP/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (100, 100) but got shape (10, 100) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.apply(params, x)  # 1.5 s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2482\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2480\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2482\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/IPython/core/magics/execution.py:1209\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1208\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[0;32m-> 1209\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mRNNCell.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h, h \u001b[38;5;66;03m# Return the new carry and the output   \u001b[39;00m\n\u001b[1;32m     22\u001b[0m scan_update \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m     23\u001b[0m     update,\n\u001b[1;32m     24\u001b[0m     variable_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m     in_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     26\u001b[0m     out_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscan_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/flax/core/axes_scan.py:148\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn\u001b[0;34m(broadcast_in, init, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m f_flat, out_tree \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mapi_util\u001b[38;5;241m.\u001b[39mflatten_fun_nokwargs(\n\u001b[1;32m    145\u001b[0m   lu\u001b[38;5;241m.\u001b[39mwrap_init(broadcast_body), in_tree\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m in_pvals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(pe\u001b[38;5;241m.\u001b[39mPartialVal\u001b[38;5;241m.\u001b[39munknown, in_avals))\n\u001b[0;32m--> 148\u001b[0m _, out_pvals, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pv, const \u001b[38;5;129;01min\u001b[39;00m out_pvals:\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/flax/core/axes_scan.py:120\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn.<locals>.body_fn\u001b[0;34m(c, xs, init_mode)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbody_fn\u001b[39m(c, xs, init_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;66;03m# inject constants\u001b[39;00m\n\u001b[1;32m    117\u001b[0m   xs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m ax, arg, x: (arg \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m broadcast \u001b[38;5;28;01melse\u001b[39;00m x), in_axes, args, xs\n\u001b[1;32m    119\u001b[0m   )\n\u001b[0;32m--> 120\u001b[0m   broadcast_out, c, ys \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcast_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m init_mode:\n\u001b[1;32m    123\u001b[0m     ys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m    124\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m ax, y: (y \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m broadcast \u001b[38;5;28;01melse\u001b[39;00m ()), out_axes, ys\n\u001b[1;32m    125\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mRNNCell.__call__.<locals>.update\u001b[0;34m(self, h, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, h, x):\n\u001b[0;32m---> 19\u001b[0m     h \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtanh(jnp\u001b[38;5;241m.\u001b[39mdot(h, Wh) \u001b[38;5;241m+\u001b[39m \u001b[43mdense_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h, h\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/flax/linen/linear.py:260\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    267\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    268\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[1;32m    269\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/flax/core/scope.py:964\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m    959\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 964\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    965\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[1;32m    966\u001b[0m       )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (100, 100) but got shape (10, 100) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "%timeit model.apply(params, x)  # 1.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86 s ± 124 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "jit_model = jax.jit(model.apply)\n",
    "%timeit jit_model(params, x)  # 1.5 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                RNNCell Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ RNNCell │ \u001b[2mfloat32\u001b[0m[5,10] │ - \u001b[2mfloat32\u001b[0m[10]   │ W: \u001b[2mfloat32\u001b[0m[10,10]      │\n",
      "│         │         │               │ - \u001b[2mfloat32\u001b[0m[5,10] │                        │\n",
      "│         │         │               │                 │ \u001b[1m100 \u001b[0m\u001b[1;2m(400 B)\u001b[0m            │\n",
      "├─────────┼─────────┼───────────────┼─────────────────┼────────────────────────┤\n",
      "│ Dense_0 │ Dense   │ \u001b[2mfloat32\u001b[0m[10]   │ \u001b[2mfloat32\u001b[0m[10]     │ bias: \u001b[2mfloat32\u001b[0m[10]      │\n",
      "│         │         │               │                 │ kernel: \u001b[2mfloat32\u001b[0m[10,10] │\n",
      "│         │         │               │                 │                        │\n",
      "│         │         │               │                 │ \u001b[1m110 \u001b[0m\u001b[1;2m(440 B)\u001b[0m            │\n",
      "├─────────┼─────────┼───────────────┼─────────────────┼────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m210 \u001b[0m\u001b[1;2m(840 B)\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴─────────┴───────────────┴─────────────────┴────────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                         Total Parameters: 210 \u001b[0m\u001b[1;2m(840 B)\u001b[0m\u001b[1m                          \u001b[0m\n",
      "\n",
      "\n",
      "{'params': {'Dense_0': {'bias': (100,), 'kernel': (100, 100)}, 'W': (100, 100)}}\n"
     ]
    }
   ],
   "source": [
    "# use the tabulate function to see the number of parameters\n",
    "x = jnp.ones((5, 10))  # 5 timesteps, input size 10\n",
    "tabulate_fn = nn.tabulate(RNNCell(), jax.random.PRNGKey(0))\n",
    "print(tabulate_fn(x))\n",
    "print(jax.tree_map(lambda x: x.shape, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrix multiplication: \n",
    "- 10000 x (10000 multiplications + 9999 additions) = 100,000,000 multiplications + 99,990,000 additions\n",
    "- 200M operations - 24ms = 200M / 24ms = 8.33 GFLOPS\n",
    "- memory: 10000x10000 x 4 bytes * 3 matrices = 1.2GB\n",
    "- 5000x(5000 multiplications + 4999 additions) = 25,000,000 multiplications + 24,995,000 additions\n",
    "- 50M / 3.39ms = 14.75 GFLOPS\n",
    "- memory: 5000x5000 x 4 bytes * 3 matrices = 600MB\n",
    "- 2048x(2048 multiplications + 2047 additions) = 4'194'304 multiplications + 4'192'256 additions \n",
    "- 8'386'560 / 210us = 39.94 GFLOPS\n",
    "- 2000x(2000 multiplications + 1999 additions) = 4,000,000 multiplications + 3,998,000 additions\n",
    "- 8M / 205us = 39.02 GFLOPS\n",
    "- 1500x(1500 multiplications + 1499 additions) = 2,250,000 multiplications + 2,248,500 additions\n",
    "- 4.5M / 160us = 28.13 GFLOPS\n",
    "- 1024x(1024 multiplications + 1023 additions) = 1,048,576 multiplications + 1,047,552 additions\n",
    "- 2'096'128 / 37us = 56.65 GFLOPS \n",
    "- memory: 1024x1024 x 4 bytes * 3 matrices = 12MB \n",
    "- 1000 x (1000 multiplications + 999 additions) = 1,000,000 multiplications + 999,000 additions\n",
    "- 1'999'000 / 34.5us = 58.03 GFLOPS\n",
    "- 500x(500 multiplications + 499 additions) = 250,000 multiplications + 249,500 additions\n",
    "- 500k / 45us = 11.11 GFLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.ones((10000, 10000))\n",
    "B = jnp.ones((10000, 10000))\n",
    "def matmul(A, B):\n",
    "    return jnp.dot(A, B)\n",
    "jit_matmul = jax.jit(matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = jit_matmul(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.1 ms ± 21.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%timeit jit_matmul(A, B)  # 1.5 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking the softmax function of a vector\n",
    "- x of size N\n",
    "- exp(x) = N flop\n",
    "- sum(exp(x)) = N-1 flop (can be highly optimized)\n",
    "- exp(x) / sum(exp(x)) = N flop\n",
    "- Total flop = 3N flop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit_softmax = jax.jit(jax.nn.softmax)\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (500000,))\n",
    "a = jit_softmax(x)  # warmup\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 μs ± 45.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jax.nn.softmax(x)\n",
    "%timeit jit_softmax(x)  # 1.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.PRNGKey(0), (500000,))\n",
    "cpu_softmax = jax.jit(jax.nn.softmax, device=jax.devices(\"cpu\")[0])\n",
    "result = cpu_softmax(x)\n",
    "gpu_softmax = jax.jit(jax.nn.softmax, device=jax.devices(\"gpu\")[0])\n",
    "result = gpu_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 μs ± 60.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "30.9 μs ± 1.54 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cpu_softmax(x)  # \n",
    "%timeit gpu_softmax(x)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "jit_hand_softmax = jax.jit(hand_softmax)\n",
    "result = jit_hand_softmax(x)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.5 μs ± 2.77 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "27.9 μs ± 1.89 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%timeit hand_softmax(x)  \n",
    "%timeit jit_hand_softmax(x)  # always 10% faster than jax.nn.softmax, surprisingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_conda_env_LearningJAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
