{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook focuses on flax.linen.scan function, key for any recurrent model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I show: \n",
    "- Two ways of implementing a custom initialization for a Flax model.\n",
    "- How to use the `nn.scan` function to implement a simple RNN.\n",
    "- How to use the `tabulate` function to display the model's parameters.\n",
    "- How to compute the flop of this model. \n",
    "- Maximum FLOPs achievable by the GPU through JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement: \n",
    "\n",
    "$\n",
    "h_t = \\tanh(W_{hh} h_{t-1} + (W_{xh} x_t + b_h))\n",
    "$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, $W_{hh}$, $W_{xh}$ and $b_h$ are the weights and bias of the RNN.\n",
    "\n",
    "- $W_{hh} h_{t-1}$ is computed via an explicit matrix multiplication (`jnp.dot`, actually we compute $h_{t-1} W_{hh}$).\n",
    "- $(W_{xh} x_t + b_h)$ is computed via a `nn.Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I go, `nn.scan` can only be used inside a __call__ method on a function also defined inside the __call__ method. \n",
    "\n",
    "Also, the scanned function must use the signature `fn(self, carry, x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (20, 10)\n"
     ]
    }
   ],
   "source": [
    "def custom_w_init():\n",
    "    def init(rng, shape):\n",
    "        return jax.random.uniform(rng, shape, minval=-0.1, maxval=0.1)\n",
    "    return init\n",
    "\n",
    "def another_custom_w_init(rng, shape, dtype=jnp.float32):\n",
    "    return jax.random.uniform(rng, shape, minval=-0.1, maxval=0.1, dtype=dtype)\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    hidden_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        Wh = self.param('W', custom_w_init(), (self.hidden_dim, self.hidden_dim))\n",
    "        dense_in = nn.Dense(features=self.hidden_dim, kernel_init=another_custom_w_init)\n",
    "        h = jnp.zeros((self.hidden_dim,))\n",
    "\n",
    "        def update(self, h, x):\n",
    "            h = jnp.tanh(jnp.dot(h, Wh) + dense_in(x))\n",
    "            return h, h # Return the new carry and the output   \n",
    "        \n",
    "        scan_update = nn.scan(\n",
    "            update,\n",
    "            variable_broadcast='params',\n",
    "            in_axes=0,\n",
    "            out_axes=0\n",
    "        )\n",
    "        \n",
    "        return scan_update(self, h, x)\n",
    "\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "x = jnp.ones((20, 100))  # 5 timesteps, input size 10\n",
    "HIDDEN_DIM = 10\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key)\n",
    "model = RNNCell(HIDDEN_DIM)\n",
    "params = model.init(key, x)\n",
    "out, hist = model.apply(params, x)\n",
    "print(out.shape, hist.shape)  # (10,) (20, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (200000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "HIDDEN_DIM = 100\n",
    "x = jnp.ones((200000, HIDDEN_DIM))  # 5 timesteps, input size 10\n",
    "\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = RNNCell(hidden_dim=HIDDEN_DIM)\n",
    "params = model.init(key, x)\n",
    "out, hist = model.apply(params, x)\n",
    "print(out.shape, hist.shape)  # (5, 10) (5, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute the flops of this RNN\n",
    "- $W_h h_{t-1}$ is a matrix multiplication of size 100x100 -> 100x(100 mults + 99 adds) = 19'900 flops\n",
    "- $W_x x_t$ is a matrix multiplication of size 100x100 -> 100x(100 mults + 99 adds) = 19'900 flops\n",
    "- $W_h h_{t-1} + W_x x_t$ is an addition of two vectors of size 100 -> 100 adds = 100 flops\n",
    "- The activation function is 100 flops --> 100 flop\n",
    "- The total flop per time step is 19'900 + 19'900 + 100 + 100 = 39'000 flop\n",
    "- We do 200'000 time steps --> 200'000 * 39'000 = 7'800'000'000 flop\n",
    "- The runtime is 1.95s --> 7'800'000'000 flop / 1.95s = 4'000'000'000 flop/s = 4 GFLOP/s\n",
    "- The jit runtime is 1.85s --> 7'800'000'000 flop / 1.85s = 4'216'216'216 flop/s = 4.2 GFLOP/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.89 s ± 57.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.apply(params, x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.83 s ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "jit_model = jax.jit(model.apply)\n",
    "%timeit jit_model(params, x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                RNNCell Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ RNNCell │ \u001b[2mfloat32\u001b[0m[5,10] │ - \u001b[2mfloat32\u001b[0m[10]   │ W: \u001b[2mfloat32\u001b[0m[10,10]      │\n",
      "│         │         │               │ - \u001b[2mfloat32\u001b[0m[5,10] │                        │\n",
      "│         │         │               │                 │ \u001b[1m100 \u001b[0m\u001b[1;2m(400 B)\u001b[0m            │\n",
      "├─────────┼─────────┼───────────────┼─────────────────┼────────────────────────┤\n",
      "│ Dense_0 │ Dense   │ \u001b[2mfloat32\u001b[0m[10]   │ \u001b[2mfloat32\u001b[0m[10]     │ bias: \u001b[2mfloat32\u001b[0m[10]      │\n",
      "│         │         │               │                 │ kernel: \u001b[2mfloat32\u001b[0m[10,10] │\n",
      "│         │         │               │                 │                        │\n",
      "│         │         │               │                 │ \u001b[1m110 \u001b[0m\u001b[1;2m(440 B)\u001b[0m            │\n",
      "├─────────┼─────────┼───────────────┼─────────────────┼────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m210 \u001b[0m\u001b[1;2m(840 B)\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴─────────┴───────────────┴─────────────────┴────────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                         Total Parameters: 210 \u001b[0m\u001b[1;2m(840 B)\u001b[0m\u001b[1m                          \u001b[0m\n",
      "\n",
      "\n",
      "{'params': {'Dense_0': {'bias': (100,), 'kernel': (100, 100)}, 'W': (100, 100)}}\n"
     ]
    }
   ],
   "source": [
    "# use the tabulate function to see the number of parameters\n",
    "x = jnp.ones((5, 10))  # 5 timesteps, input size 10\n",
    "tabulate_fn = nn.tabulate(RNNCell(), jax.random.PRNGKey(0))\n",
    "print(tabulate_fn(x))\n",
    "print(jax.tree_map(lambda x: x.shape, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix multiplication: \n",
    "Assume $N$ is the size of the matrix. Let `bs` be the batch size.\n",
    "Then we have: $ bs \\text{ matrix multiplications } * N \\text{ rows } * (N \\text{ multiplications } + (N-1) \\text{ additions}) = bs * N * (2N-1) \\text{ flop} $\n",
    "\n",
    "When we use vmap, we actually do `bs` matrix multiplications in parallel which means that the memory usage passes from $N^2 * 3$ to $(2*bs + 1) N^2$ because the batched tensor is dispatched, every dispatch produces a result while the common tensor is shared between all the dispatches.\n",
    "\n",
    "| N     | bs    | Flop          | Time     | Flops        | Memory       |\n",
    "|-------|-------|---------------|----------|--------------|--------------|\n",
    "| 10000 | 1     |   199'990'000 |    24 ms |  8.33 GFLOPS |  1.12 GB     |\n",
    "| 5000  | 1     |    49'995'000 |  3.39 ms | 14.75 GFLOPS |   286 MB     |\n",
    "| 2048  | 1     |     8'386'560 |   210 us | 39.94 GFLOPS |    48 MB     |\n",
    "| 2000  | 1     |     7'998'000 |   205 us | 39.02 GFLOPS |    46 MB     |\n",
    "| 1500  | 1     |     4'498'500 |   160 us | 28.13 GFLOPS |    26 MB     |\n",
    "| 1024  | 1     |     2'096'128 |    37 us | 56.65 GFLOPS |    12 MB     |\n",
    "| 1000  | 1     |     1'999'000 |  34.5 us | 58.03 GFLOPS |    11 MB     |\n",
    "| 500   | 1     |       500'000 |    45 us | 11.11 GFLOPS |     3 MB     |\n",
    "| 1000  | 100   |   199'900'000 |  2.53 ms | 79.1  GFLOPS |   766 MB     |\n",
    "| 1000  | 1000  | 1'999'000'000 |  24.5 ms | 81.63 GFLOPS |  7.45 GB     |\n",
    "| 1024  | 1024  | 2'146'435'072 |  26.1 ms | 82.24 GFLOPS |     8 GB     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199990000 - 8.3329e+09 - 1.12 GB - 1144.41 MB - 1171875.00 KB\n",
      "49995000 - 1.4748e+10 - 0.28 GB - 286.10 MB - 292968.75 KB\n",
      "8386560 - 3.9936e+10 - 0.05 GB - 48.00 MB - 49152.00 KB\n",
      "7998000 - 3.9015e+10 - 0.04 GB - 45.78 MB - 46875.00 KB\n",
      "4498500 - 2.8116e+10 - 0.03 GB - 25.75 MB - 26367.19 KB\n",
      "2096128 - 5.6652e+10 - 0.01 GB - 12.00 MB - 12288.00 KB\n",
      "1999000 - 5.8794e+10 - 0.01 GB - 11.44 MB - 11718.75 KB\n",
      "499500 - 1.1100e+10 - 0.00 GB - 2.86 MB - 2929.69 KB\n",
      "199900000 - 7.9012e+10 - 0.75 GB - 766.75 MB - 785156.25 KB\n",
      "1999000000 - 8.1592e+10 - 7.45 GB - 7633.21 MB - 7816406.25 KB\n",
      "2146435072 - 8.2239e+10 - 8.00 GB - 8196.00 MB - 8392704.00 KB\n"
     ]
    }
   ],
   "source": [
    "def flop_compute(N, bs):\n",
    "    return bs * N * (2*N - 1)\n",
    "def memory(N, bs):\n",
    "    return ((2*bs + 1) * N**2) * 4 \n",
    "def flops_compute(N, bs, time_in_s):\n",
    "    a = flop_compute(N, bs)\n",
    "    b = memory(N, bs)\n",
    "    print(f'{a} - {a/time_in_s:.4e} - {b/(1024**3):.2f} GB - {b/(1024**2):.2f} MB - {b/(1024):.2f} KB')\n",
    "\n",
    "flops_compute(10000, 1, 0.024)\n",
    "flops_compute(5000, 1, 0.00339)\n",
    "flops_compute(2048, 1, 0.000210)\n",
    "flops_compute(2000, 1, 0.000205)\n",
    "flops_compute(1500, 1, 0.000160)\n",
    "flops_compute(1024, 1, 0.000037)\n",
    "flops_compute(1000, 1, 0.000034)\n",
    "flops_compute(500, 1, 0.000045)\n",
    "flops_compute(1000, 100, 0.00253)\n",
    "flops_compute(1000, 1000, 0.0245)\n",
    "flops_compute(1024, 1024, 0.0261)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "A = jnp.ones((N, N))\n",
    "B = jnp.ones((N, N))\n",
    "def matmul(A, B):\n",
    "    return jnp.dot(A, B)\n",
    "jit_matmul = jax.jit(matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = jit_matmul(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.3 μs ± 40.9 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_matmul(A, B)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "C = jnp.ones((bs, N, N))\n",
    "jit_vmap = jax.jit(jax.vmap(matmul, in_axes=(0, None), out_axes=0))\n",
    "_ = jit_vmap(C, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 ms ± 1.32 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_vmap(C, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap_jit = jax.vmap(jax.jit(matmul), in_axes=(0, None), out_axes=0)\n",
    "_ = vmap_jit(C, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 ms ± 26.6 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vmap_jit(C, B)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvm(A, b):\n",
    "    return jnp.dot(A, b)\n",
    "\n",
    "jit_mvm = jax.jit(mvm)\n",
    "vmap_jit_mvm = jax.vmap(jit_mvm, in_axes=(None, 0), out_axes=0)\n",
    "_ = vmap_jit_mvm(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 37.37 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3.31 ms ± 6.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vmap_jit_mvm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_vmap_mvm = jax.jit(jax.vmap(mvm, in_axes=(None, 0), out_axes=0))\n",
    "_ = jit_vmap_mvm(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.1 μs ± 8.06 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_vmap_mvm(A, B)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking the softmax function of a vector\n",
    "- x of size N\n",
    "- exp(x) = N flop\n",
    "- sum(exp(x)) = N-1 flop (can be highly optimized)\n",
    "- exp(x) / sum(exp(x)) = N flop\n",
    "- Total flop = 3N flop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit_softmax = jax.jit(jax.nn.softmax)\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (500000,))\n",
    "a = jit_softmax(x)  # warmup\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 μs ± 45.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jax.nn.softmax(x)\n",
    "%timeit jit_softmax(x)  # 1.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.PRNGKey(0), (500000,))\n",
    "cpu_softmax = jax.jit(jax.nn.softmax, device=jax.devices(\"cpu\")[0])\n",
    "result = cpu_softmax(x)\n",
    "gpu_softmax = jax.jit(jax.nn.softmax, device=jax.devices(\"gpu\")[0])\n",
    "result = gpu_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 μs ± 60.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "30.9 μs ± 1.54 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cpu_softmax(x)  # \n",
    "%timeit gpu_softmax(x)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "jit_hand_softmax = jax.jit(hand_softmax)\n",
    "result = jit_hand_softmax(x)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.5 μs ± 2.77 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "27.9 μs ± 1.89 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%timeit hand_softmax(x)  \n",
    "%timeit jit_hand_softmax(x)  # always 10% faster than jax.nn.softmax, surprisingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_conda_env_LearningJAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
