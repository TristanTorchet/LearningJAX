{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook focuses on flax.linen.scan function, key for any recurrent model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I show: \n",
    "- Two ways of implementing a custom initialization for a Flax model.\n",
    "- How to use the `nn.scan` function to implement a simple RNN.\n",
    "- How to use the `tabulate` function to display the model's parameters.\n",
    "- How to compute the flop of this model. \n",
    "- Maximum FLOPs achievable by the GPU through JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement: \n",
    "\n",
    "$\n",
    "h_t = \\tanh(W_{hh} h_{t-1} + (W_{xh} x_t + b_h))\n",
    "$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, $W_{hh}$, $W_{xh}$ and $b_h$ are the weights and bias of the RNN.\n",
    "\n",
    "- $W_{hh} h_{t-1}$ is computed via an explicit matrix multiplication (`jnp.dot`, actually we compute $h_{t-1} W_{hh}$).\n",
    "- $(W_{xh} x_t + b_h)$ is computed via a `nn.Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# jax memory allocation\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I go, `nn.scan` can only be used inside a __call__ method on a function also defined inside the __call__ method. \n",
    "\n",
    "Also, the scanned function must use the signature `fn(self, carry, x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_w_init():\n",
    "    def init(rng, shape):\n",
    "        return jax.random.uniform(rng, shape, minval=-0.1, maxval=0.1)\n",
    "    return init\n",
    "\n",
    "def another_custom_w_init(rng, shape, dtype=jnp.float32):\n",
    "    return jax.random.uniform(rng, shape, minval=-0.1, maxval=0.1, dtype=dtype)\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    hidden_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        Wh = self.param('W', custom_w_init(), (self.hidden_dim, self.hidden_dim))\n",
    "        dense_in = nn.Dense(features=self.hidden_dim, kernel_init=another_custom_w_init)\n",
    "        h = jnp.zeros((self.hidden_dim,))\n",
    "\n",
    "        def update(self, h, x):\n",
    "            h = jnp.tanh(jnp.dot(h, Wh) + dense_in(x))\n",
    "            return h, h # Return the new carry and the output   \n",
    "        \n",
    "        scan_update = nn.scan(\n",
    "            update,\n",
    "            variable_broadcast='params',\n",
    "            in_axes=0,\n",
    "            out_axes=0\n",
    "        )\n",
    "        \n",
    "        return scan_update(self, h, x)\n",
    "\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "x = jnp.ones((20, 100))  # 5 timesteps, input size 10\n",
    "HIDDEN_DIM = 10\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key)\n",
    "model = RNNCell(HIDDEN_DIM)\n",
    "params = model.init(key, x)\n",
    "out, hist = model.apply(params, x)\n",
    "print(out.shape, hist.shape)  # (10,) (20, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (200000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "HIDDEN_DIM = 100\n",
    "x = jnp.ones((200000, HIDDEN_DIM))  # 5 timesteps, input size 10\n",
    "\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = RNNCell(hidden_dim=HIDDEN_DIM)\n",
    "params = model.init(key, x)\n",
    "out, hist = model.apply(params, x)\n",
    "print(out.shape, hist.shape)  # (5, 10) (5, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute the flops of this RNN\n",
    "- $W_h h_{t-1}$ is a matrix multiplication of size 100x100 -> 100x(100 mults + 99 adds) = 19'900 flops\n",
    "- $W_x x_t$ is a matrix multiplication of size 100x100 -> 100x(100 mults + 99 adds) = 19'900 flops\n",
    "- $W_h h_{t-1} + W_x x_t$ is an addition of two vectors of size 100 -> 100 adds = 100 flops\n",
    "- The activation function is 100 flops --> 100 flop\n",
    "- The total flop per time step is 19'900 + 19'900 + 100 + 100 = 39'000 flop\n",
    "- We do 200'000 time steps --> 200'000 * 39'000 = 7'800'000'000 flop\n",
    "- The runtime is 1.95s --> 7'800'000'000 flop / 1.95s = 5'650'000'000 flop/s = 5.65 GFLOP/s\n",
    "- The jit runtime is 1.35s --> 7'800'000'000 flop / 1.85s = 5'820'895'522 flop/s = 5.82 GFLOP/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38 s ± 14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.apply(params, x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 s ± 10.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "jit_model = jax.jit(model.apply)\n",
    "%timeit jit_model(params, x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                RNNCell Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ RNNCell │ \u001b[2mfloat32\u001b[0m[5,10] │ - \u001b[2mfloat32\u001b[0m[10]   │ W: \u001b[2mfloat32\u001b[0m[10,10]      │\n",
      "│         │         │               │ - \u001b[2mfloat32\u001b[0m[5,10] │                        │\n",
      "│         │         │               │                 │ \u001b[1m100 \u001b[0m\u001b[1;2m(400 B)\u001b[0m            │\n",
      "├─────────┼─────────┼───────────────┼─────────────────┼────────────────────────┤\n",
      "│ Dense_0 │ Dense   │ \u001b[2mfloat32\u001b[0m[10]   │ \u001b[2mfloat32\u001b[0m[10]     │ bias: \u001b[2mfloat32\u001b[0m[10]      │\n",
      "│         │         │               │                 │ kernel: \u001b[2mfloat32\u001b[0m[10,10] │\n",
      "│         │         │               │                 │                        │\n",
      "│         │         │               │                 │ \u001b[1m110 \u001b[0m\u001b[1;2m(440 B)\u001b[0m            │\n",
      "├─────────┼─────────┼───────────────┼─────────────────┼────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m210 \u001b[0m\u001b[1;2m(840 B)\u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴─────────┴───────────────┴─────────────────┴────────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                         Total Parameters: 210 \u001b[0m\u001b[1;2m(840 B)\u001b[0m\u001b[1m                          \u001b[0m\n",
      "\n",
      "\n",
      "{'params': {'Dense_0': {'bias': (100,), 'kernel': (100, 100)}, 'W': (100, 100)}}\n"
     ]
    }
   ],
   "source": [
    "# use the tabulate function to see the number of parameters\n",
    "x = jnp.ones((5, 10))  # 5 timesteps, input size 10\n",
    "tabulate_fn = nn.tabulate(RNNCell(), jax.random.PRNGKey(0))\n",
    "print(tabulate_fn(x))\n",
    "print(jax.tree_map(lambda x: x.shape, params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_conda_env_LearningJAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
