{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX vs PyTorch\n",
    "\n",
    "# 1. JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# jax memory allocation\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.95'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication: \n",
    "\n",
    "### Square matrices:\n",
    "\n",
    "#### Flop:\n",
    "`A (bs, N, N)` and `B (N, N)` are two square matrices of size `N x N`.\n",
    "\n",
    "Then we have: \n",
    "\n",
    "$ bs \\text{ matrix multiplications } * N \\text{ rows } * N \\text{ columns } * (N \\text{ multiplications } + (N-1) \\text{ additions})$\n",
    "\n",
    "$= bs * N^2 * (2N-1) \\text{ flop} $\n",
    "\n",
    "#### Memory: \n",
    "When we use vmap, we actually do `bs` matrix multiplications in parallel which means that the memory usage passes from $N^2 * 3$ to $(2*bs + 1) N^2$ because the batched tensor is dispatched, every dispatch produces a result while the common tensor is shared between all the dispatches.\n",
    "\n",
    "### Rectangular matrices:\n",
    "`A (bs, N, M)` and `B (M, P)` \n",
    "we have: \n",
    "$ bs \\text{ matrix multiplications } * N \\text{ rows } * P \\text{ columns } * (M \\text{ multiplications } + (M-1) \\text{ additions}) $\n",
    "\n",
    "$= bs * N * P * (2M-1) \\text{ flop} $\n",
    "\n",
    "### Results for Square matrices:\n",
    "\n",
    "| N     | bs    | Flop              | Time     | Flops        | Memory       |\n",
    "|-------|-------|-------------------|----------|--------------|--------------|\n",
    "| 10000 | 1     |   199'990'000'000 |    24 ms | 83.33 TFLOPS |  1.12 GB     |\n",
    "| 5000  | 1     |   249'975'000'000 |  3.39 ms | 73.74 TFLOPS |   286 MB     |\n",
    "| 2048  | 1     |    17'175'674'880 |   210 us | 81.79 TFLOPS |    48 MB     |\n",
    "| 2000  | 1     |    15'996'000'000 |   205 us | 78.03 TFLOPS |    46 MB     |\n",
    "| 1500  | 1     |     6'747'750'000 |   160 us | 41.17 TFLOPS |    26 MB     |\n",
    "| 1024  | 1     |     2'146'435'072 |    37 us | 58.01 TFLOPS |    12 MB     |\n",
    "| 1000  | 1     |     1'999'000'000 |  34.5 us | 58.79 TFLOPS |    11 MB     |\n",
    "| 500   | 1     |       249'750'000 |    45 us |   5.5 TFLOPS |     3 MB     |\n",
    "| 1000  | 100   |   199'900'000'000 |  2.53 ms | 79.01 TFLOPS |   766 MB     |\n",
    "| 1000  | 1000  | 1'999'000'000'000 |  24.5 ms | 81.59 TFLOPS |  7.45 GB     |\n",
    "| 1024  | 1024  | 2'197'949'513'728 |  26.1 ms | 84.12 TFLOPS |     8 GB     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000  | 1     | 1'999'900'000'000 |  83.33 | 1.12 GB - 1144.41 MB - 1171875.00 KB\n",
      "5000   | 1     |   249'975'000'000 |  73.74 | 0.28 GB - 286.10 MB - 292968.75 KB\n",
      "2048   | 1     |    17'175'674'880 |  81.79 | 0.05 GB - 48.00 MB - 49152.00 KB\n",
      "2000   | 1     |    15'996'000'000 |  78.03 | 0.04 GB - 45.78 MB - 46875.00 KB\n",
      "1500   | 1     |     6'747'750'000 |  42.17 | 0.03 GB - 25.75 MB - 26367.19 KB\n",
      "1024   | 1     |     2'146'435'072 |  58.01 | 0.01 GB - 12.00 MB - 12288.00 KB\n",
      "1000   | 1     |     1'999'000'000 |  58.79 | 0.01 GB - 11.44 MB - 11718.75 KB\n",
      "500    | 1     |       249'750'000 |   5.55 | 0.00 GB - 2.86 MB - 2929.69 KB\n",
      "1000   | 100   |   199'900'000'000 |  79.01 | 0.75 GB - 766.75 MB - 785156.25 KB\n",
      "1000   | 1000  | 1'999'000'000'000 |  81.59 | 7.45 GB - 7633.21 MB - 7816406.25 KB\n",
      "1024   | 1024  | 2'197'949'513'728 |  84.21 | 8.00 GB - 8196.00 MB - 8392704.00 KB\n"
     ]
    }
   ],
   "source": [
    "def flop_compute(N, bs):\n",
    "    return bs * N**2 * (2*N - 1)\n",
    "def memory(N, bs):\n",
    "    return ((2*bs + 1) * N**2) * 4 \n",
    "def format_nb(nb):\n",
    "    return f'{nb:,}'.replace(',', \"'\")\n",
    "def flops_compute(N, bs, time_in_s):\n",
    "    a = flop_compute(N, bs)\n",
    "    b = memory(N, bs)\n",
    "    print(f'{N:<6} | {bs:<5} | {format_nb(a):>17} | {(a/time_in_s)/1e12:>6.4} | {b/(1024**3):.2f} GB - {b/(1024**2):.2f} MB - {b/(1024):.2f} KB')\n",
    "\n",
    "flops_compute(10000,    1, 0.024)\n",
    "flops_compute( 5000,    1, 0.00339)\n",
    "flops_compute( 2048,    1, 0.000210)\n",
    "flops_compute( 2000,    1, 0.000205)\n",
    "flops_compute( 1500,    1, 0.000160)\n",
    "flops_compute( 1024,    1, 0.000037)\n",
    "flops_compute( 1000,    1, 0.000034)\n",
    "flops_compute(  500,    1, 0.000045)\n",
    "flops_compute( 1000,  100, 0.00253)\n",
    "flops_compute( 1000, 1000, 0.0245)\n",
    "flops_compute( 1024, 1024, 0.0261)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "A = jnp.ones((N, N))\n",
    "B = jnp.ones((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A, B):\n",
    "    return jnp.dot(A, B)\n",
    "jit_matmul = jax.jit(matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = jit_matmul(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.3 μs ± 9.16 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_matmul(A, B)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='green'>SOLVED: ```jax.lib.xla_bridge.get_backend()```</font>**\n",
    "\n",
    "<font color='red'>I DON'T UNDERSTAND</font>\n",
    "\n",
    "This code run:\n",
    "```python\n",
    "bs = 1024\n",
    "N = 1024\n",
    "B = jnp.ones((N, N))\n",
    "C = jnp.ones((bs, N, N))\n",
    "jit_vmap = jax.jit(jax.vmap(matmul, in_axes=(0, None), out_axes=0))\n",
    "_ = jit_vmap(C, B)  # warmup\n",
    "```\n",
    "\n",
    "<font color='red'> But if I restart the kernel and run this code, I get OOM for the second part of the following code (which is exactly the same as the one above)</font>\n",
    "```python\n",
    "bs = 2048\n",
    "N = 1024\n",
    "B = jnp.ones((N, N))\n",
    "C = jnp.ones((bs, N, N))\n",
    "jit_vmap = jax.jit(jax.vmap(matmul, in_axes=(0, None), out_axes=0))\n",
    "_ = jit_vmap(C, B)  # warmup\n",
    ">>> RuntimeError: Resource exhausted: Out of memory while trying to allocate 8.00GiB\n",
    "\n",
    "bs = 1024\n",
    "N = 1024\n",
    "B = jnp.ones((N, N))\n",
    "C = jnp.ones((bs, N, N))\n",
    "jit_vmap = jax.jit(jax.vmap(matmul, in_axes=(0, None), out_axes=0))\n",
    "_ = jit_vmap(C, B)  # warmup\n",
    ">>> RuntimeError: Resource exhausted: Out of memory while trying to allocate 4.00GiB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "backend = jax.lib.xla_bridge.get_backend()\n",
    "print(len(backend.live_buffers()))\n",
    "for buf in backend.live_buffers(): buf.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting C (1024, 1024, 1024)\n",
      "Deleting jit_vmap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 12:28:09.618073: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6459228160 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n",
      "\u001b[1;32m     15\u001b[0m C \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((bs, N, N))\n",
      "\u001b[1;32m     16\u001b[0m jit_vmap \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(jax\u001b[38;5;241m.\u001b[39mvmap(matmul, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), out_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;32m---> 17\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mjit_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# warmup\u001b[39;00m\n",
      "\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/jax/_src/compiler.py:237\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n",
      "\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n",
      "\u001b[1;32m    233\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n",
      "\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n",
      "\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n",
      "\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n",
      "\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6459228160 bytes."
     ]
    }
   ],
   "source": [
    "jax.clear_caches()\n",
    "\n",
    "# if C exists delete it\n",
    "if 'C' in locals():\n",
    "    print('Deleting C', C.shape)\n",
    "    del C, B\n",
    "\n",
    "if 'jit_vmap' in locals():\n",
    "    print('Deleting jit_vmap')\n",
    "    del jit_vmap\n",
    "\n",
    "bs = 1024 + 512\n",
    "N = 1024\n",
    "B = jnp.ones((N, N))\n",
    "C = jnp.ones((bs, N, N))\n",
    "jit_vmap = jax.jit(jax.vmap(matmul, in_axes=(0, None), out_axes=0))\n",
    "_ = jit_vmap(C, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 ms ± 4.47 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_vmap(C, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap_jit = jax.vmap(jax.jit(matmul), in_axes=(0, None), out_axes=0)\n",
    "_ = vmap_jit(C, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 ms ± 3.21 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vmap_jit(C, B)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvm(A, b):\n",
    "    return jnp.dot(A, b)\n",
    "\n",
    "jit_mvm = jax.jit(mvm)\n",
    "vmap_jit_mvm = jax.vmap(jit_mvm, in_axes=(None, 0), out_axes=0)\n",
    "_ = vmap_jit_mvm(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 μs ± 12.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vmap_jit_mvm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_vmap_mvm = jax.jit(jax.vmap(mvm, in_axes=(None, 0), out_axes=0))\n",
    "_ = jit_vmap_mvm(A, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.5 μs ± 31.9 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_vmap_mvm(A, B)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking the softmax function of a vector\n",
    "- x of size N\n",
    "- exp(x) = N flop\n",
    "- sum(exp(x)) = N-1 flop (can be highly optimized)\n",
    "- exp(x) / sum(exp(x)) = N flop\n",
    "- Total flop = 3N flop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jit_softmax = jax.jit(jax.nn.softmax)\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (500000,))\n",
    "a = jit_softmax(x)  # warmup\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 μs ± 45.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jax.nn.softmax(x)\n",
    "%timeit jit_softmax(x)  # 1.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.61e+09 FLOPS\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "t = 3 * 500*1e3 / (227 * 1e-6)\n",
    "print(f'{t:.2e} FLOPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.PRNGKey(0), (500000,))\n",
    "cpu_softmax = jax.jit(jax.nn.softmax, device=jax.devices(\"cpu\")[0])\n",
    "result = cpu_softmax(x)\n",
    "gpu_softmax = jax.jit(jax.nn.softmax, device=jax.devices(\"gpu\")[0])\n",
    "result = gpu_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 μs ± 60.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "30.9 μs ± 1.54 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cpu_softmax(x)  # \n",
    "%timeit gpu_softmax(x)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "jit_hand_softmax = jax.jit(hand_softmax)\n",
    "result = jit_hand_softmax(x)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.5 μs ± 2.77 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "27.9 μs ± 1.89 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%timeit hand_softmax(x)  \n",
    "%timeit jit_hand_softmax(x)  # always 10% faster than jax.nn.softmax, surprisingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "NVIDIA GeForce RTX 4090\n",
      "cuda\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "N = 1024\n",
    "A = torch.ones((N, N)).cuda()\n",
    "B = torch.ones((N, N)).cuda()\n",
    "\n",
    "def matmul(A, B):\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "jit_matmul = torch.jit.script(matmul)\n",
    "_ = jit_matmul(A, B)  # warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 μs ± 44.2 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 μs ± 14.6 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04579520225524902\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "n = 10000\n",
    "for _ in range(n):\n",
    "    matmul(A, B)\n",
    "end = time.time()\n",
    "print(f'{((end - start) / n)*1000:.3f}') # result in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "C = torch.ones((bs, N, N)).cuda()\n",
    "\n",
    "\n",
    "vmap_jit = torch.func.vmap(jit_matmul, in_dims=(0, None), out_dims=0)\n",
    "vmap = torch.func.vmap(matmul, in_dims=(0, None), out_dims=0)\n",
    "_ = vmap_jit(C, B)  # warmup\n",
    "_ = vmap(C, B)  # warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.908361792564392\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "n = 400\n",
    "for _ in range(n):\n",
    "    vmap(C, B)\n",
    "end = time.time()\n",
    "print(((end - start) / n)*1000) # result in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotSupportedError",
     "evalue": "Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"/home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/torch/_functorch/apis.py\", line 187\n    def wrapped(*args, **kwargs):\n                        ~~~~~~~ <--- HERE\n        return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m jit_vmap \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m _ \u001b[38;5;241m=\u001b[39m jit_vmap(C, B)  \u001b[38;5;66;03m# warmup\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/torch/jit/_script.py:1392\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n",
      "\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maybe_already_compiled_fn:\n",
      "\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_already_compiled_fn\n",
      "\u001b[0;32m-> 1392\u001b[0m ast \u001b[38;5;241m=\u001b[39m \u001b[43mget_jit_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rcb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   1394\u001b[0m     _rcb \u001b[38;5;241m=\u001b[39m _jit_internal\u001b[38;5;241m.\u001b[39mcreateResolutionCallbackFromClosure(obj)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/torch/jit/frontend.py:372\u001b[0m, in \u001b[0;36mget_jit_def\u001b[0;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n",
      "\u001b[1;32m    369\u001b[0m     qualname \u001b[38;5;241m=\u001b[39m get_qualified_name(fn)\n",
      "\u001b[1;32m    370\u001b[0m     pdt_arg_types \u001b[38;5;241m=\u001b[39m type_trace_db\u001b[38;5;241m.\u001b[39mget_args_types(qualname)\n",
      "\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_def\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparsed_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_def\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_line\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdef_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mself_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdt_arg_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdt_arg_types\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/torch/jit/frontend.py:422\u001b[0m, in \u001b[0;36mbuild_def\u001b[0;34m(ctx, py_def, type_line, def_name, self_name, pdt_arg_types)\u001b[0m\n",
      "\u001b[1;32m    419\u001b[0m body \u001b[38;5;241m=\u001b[39m py_def\u001b[38;5;241m.\u001b[39mbody\n",
      "\u001b[1;32m    420\u001b[0m r \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mmake_range(py_def\u001b[38;5;241m.\u001b[39mlineno, py_def\u001b[38;5;241m.\u001b[39mcol_offset, py_def\u001b[38;5;241m.\u001b[39mcol_offset \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;32m--> 422\u001b[0m param_list \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_param_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpy_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdt_arg_types\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    423\u001b[0m return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(py_def, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/torch/jit/frontend.py:448\u001b[0m, in \u001b[0;36mbuild_param_list\u001b[0;34m(ctx, py_args, self_name, pdt_arg_types)\u001b[0m\n",
      "\u001b[1;32m    444\u001b[0m     expr \u001b[38;5;241m=\u001b[39m py_args\u001b[38;5;241m.\u001b[39mkwarg\n",
      "\u001b[1;32m    445\u001b[0m     ctx_range \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mmake_range(\n",
      "\u001b[1;32m    446\u001b[0m         expr\u001b[38;5;241m.\u001b[39mlineno, expr\u001b[38;5;241m.\u001b[39mcol_offset \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, expr\u001b[38;5;241m.\u001b[39mcol_offset \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(expr\u001b[38;5;241m.\u001b[39marg)\n",
      "\u001b[1;32m    447\u001b[0m     )\n",
      "\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(ctx_range, _vararg_kwarg_err)\n",
      "\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m py_args\u001b[38;5;241m.\u001b[39mvararg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    450\u001b[0m     expr \u001b[38;5;241m=\u001b[39m py_args\u001b[38;5;241m.\u001b[39mvararg\n",
      "\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n",
      "  File \"/home/tristan/miniconda3/envs/.jax_conda_env_LearningJAX/lib/python3.12/site-packages/torch/_functorch/apis.py\", line 187\n",
      "    def wrapped(*args, **kwargs):\n",
      "                        ~~~~~~~ <--- HERE\n",
      "        return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "jit_vmap = torch.jit.script(vmap)\n",
    "_ = jit_vmap(C, B)  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_conda_env_LearningJAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
