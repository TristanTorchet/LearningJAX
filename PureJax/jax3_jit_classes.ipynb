{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIT will  necessarily trace your self.attribute into constants so we can't jit a class method <font color='red'>if we want to modify this attribute between calls</font>\n",
    "### instead we need to jit a wrapper function that calls the class method (and we can vmap the class method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dummy counter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Counter:\n",
    "    \"\"\"A simple counter.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "    \n",
    "    def count(self) -> int:\n",
    "        \"\"\"Increments the counter and returns the new value.\"\"\"\n",
    "        self.n += 1\n",
    "        return self.n\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the counter to zero.\"\"\"\n",
    "        self.n = 0\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for i in range(3):\n",
    "    n = counter.count()\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. First tentative to speed up the code by using ```jit``` on the class method --> <font color='red'> doesn't work BUT DOESN'T RAISE AN ERROR</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = 1\n",
      "out = 1\n",
      "out = 1\n"
     ]
    }
   ],
   "source": [
    "counter.reset()\n",
    "fast_count = jax.jit(counter.count)\n",
    "for i in range(3):  # oops, it's not working as it's supposed to be\n",
    "    print(f'out = {fast_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Second tentative to speed up the code by using ```jit``` on a wrapper function that calls the class method --> <font color='green'> works: around 13x faster</font>\n",
    "##### We need to create a wrapper function that takes the class parameters as arguments, instantiate an object and call the instance method inside the wrapper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = 1\n",
      "out = 2\n",
      "out = 3\n",
      "time = 0.07 ms\n",
      "params = (Array(10000, dtype=int32, weak_type=True),)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "@partial(jax.jit, static_argnums=(1, 2,))\n",
    "def wrapper(params, n_iter=10000, log=False):\n",
    "    counter_ = Counter()\n",
    "    counter_.n = params[0]\n",
    "    for _ in range(n_iter):\n",
    "        n = counter_.count()\n",
    "        if log: jax.debug.print('out = {n}', n=n) \n",
    "    return (counter_.n,)\n",
    "\n",
    "counter.reset()\n",
    "params = (counter.n,)\n",
    "\n",
    "_ = wrapper(params, n_iter=3, log=True)\n",
    "\n",
    "# warmup\n",
    "_ = wrapper(params, n_iter=10000, log=False)\n",
    "start = time.time()\n",
    "params_updated = wrapper(params, n_iter=10000, log=False)\n",
    "end = time.time()\n",
    "print(f'time = {(end - start)*1000:.2f} ms')\n",
    "print(f'params = {params_updated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.91 ms\n",
      "out = 10000\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(10000):\n",
    "    n = counter.count()\n",
    "end = time.time()\n",
    "print(f'time = {(end - start)*1000:.2f} ms')\n",
    "print(f'out = {n}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application: Linear layer as a class to mimic Equinox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Dummy Linear layer class with ```jit``` on the class method --> <font color='red'> doesn't work BUT DOESN'T RAISE AN ERROR</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        self.weight = jax.random.normal(key, (out_features, in_features))\n",
    "        self.bias = jax.random.normal(key, (out_features,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        jax.debug.print('jitted self.weight = \\n{weight}', weight=self.weight)\n",
    "        return jnp.dot(self.weight, x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = 3\n",
    "dim_out = 4\n",
    "bs = 10\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "x = jax.random.normal(subkey, (bs, dim_in))\n",
    "layer = Linear(dim_in, dim_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jitted self.weight = \n",
      "[[ 1.1901639  -1.0996888   0.44367844]\n",
      " [ 0.5984697  -0.39189556  0.69261974]\n",
      " [ 0.46018356 -2.068578   -0.21438177]\n",
      " [-0.9898306  -0.6789304   0.27362573]]\n",
      "out = \n",
      "[[ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      " [ 3.625029    0.02590603  3.410178    0.50355005]\n",
      " [ 0.8323103  -1.398576    0.28799635  0.3267359 ]\n",
      " [ 1.7365305  -0.21044779 -1.2441818  -0.9568258 ]\n",
      " [ 3.3418374   0.42115265  0.2816336  -1.6189749 ]\n",
      " [ 3.0669456  -0.31462416  0.10747854 -2.8793561 ]\n",
      " [ 1.9252988  -0.1918292  -1.0745429  -1.1775186 ]\n",
      " [ 1.645749   -0.4092752  -2.20948    -2.3588734 ]\n",
      " [ 2.2478125  -0.71958554  2.4841554   1.0517647 ]\n",
      " [ 3.4196887   0.1534388   0.6268668  -2.0282063 ]]\n",
      "\n",
      "layer.weight = \n",
      "[[ 0.5950819  -0.5498444   0.22183922]\n",
      " [ 0.29923484 -0.19594778  0.34630987]\n",
      " [ 0.23009178 -1.034289   -0.10719088]\n",
      " [-0.4949153  -0.3394652   0.13681287]]\n",
      "jitted self.weight = \n",
      "[[ 1.1901639  -1.0996888   0.44367844]\n",
      " [ 0.5984697  -0.39189556  0.69261974]\n",
      " [ 0.46018356 -2.068578   -0.21438177]\n",
      " [-0.9898306  -0.6789304   0.27362573]]\n",
      "out = \n",
      "[[ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      " [ 3.625029    0.02590603  3.410178    0.50355005]\n",
      " [ 0.8323103  -1.398576    0.28799635  0.3267359 ]\n",
      " [ 1.7365305  -0.21044779 -1.2441818  -0.9568258 ]\n",
      " [ 3.3418374   0.42115265  0.2816336  -1.6189749 ]\n",
      " [ 3.0669456  -0.31462416  0.10747854 -2.8793561 ]\n",
      " [ 1.9252988  -0.1918292  -1.0745429  -1.1775186 ]\n",
      " [ 1.645749   -0.4092752  -2.20948    -2.3588734 ]\n",
      " [ 2.2478125  -0.71958554  2.4841554   1.0517647 ]\n",
      " [ 3.4196887   0.1534388   0.6268668  -2.0282063 ]]\n"
     ]
    }
   ],
   "source": [
    "layer_call = jax.jit(layer)\n",
    "layer_call = jax.vmap(layer_call, in_axes=(0,))\n",
    "print(f'out = \\n{layer_call(x)}\\n')\n",
    "\n",
    "# scale the weights by 2\n",
    "layer.weight = layer.weight / 2\n",
    "layer.bias = layer.bias / 2\n",
    "print(f'layer.weight = \\n{layer.weight}')\n",
    "print(f'out = \\n{layer_call(x)}') # WARNING: we can see that the output is not scaled, meaning that it still uses the old weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Taking inspiration from equinox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1: Equinox Custom Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=[ 0.00231779 -1.9878993  -1.513092   -1.1650387 ]\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "class EqxLinear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        self.weight = jax.random.normal(key, (out_features, in_features))\n",
    "        self.bias = jax.random.normal(key, (out_features,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(self.weight, x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = 3\n",
    "dim_out = 4\n",
    "bs = 10\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (bs, dim_in))\n",
    "y = jnp.ones((bs,)).astype(jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=[ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      "out = \n",
      "[[ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      " [ 3.625029    0.02590603  3.410178    0.50355005]\n",
      " [ 0.8323103  -1.398576    0.28799635  0.3267359 ]\n",
      " [ 1.7365305  -0.21044779 -1.2441818  -0.9568258 ]\n",
      " [ 3.3418374   0.42115265  0.2816336  -1.6189749 ]\n",
      " [ 3.0669456  -0.31462416  0.10747854 -2.8793561 ]\n",
      " [ 1.9252988  -0.1918292  -1.0745429  -1.1775186 ]\n",
      " [ 1.645749   -0.4092752  -2.20948    -2.3588734 ]\n",
      " [ 2.2478125  -0.71958554  2.4841554   1.0517647 ]\n",
      " [ 3.4196887   0.1534388   0.6268668  -2.0282063 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eqx_layer = EqxLinear(dim_in, dim_out)\n",
    "print(f'out={eqx_layer(x[0])}')\n",
    "print(f'out = \\n{jax.vmap(eqx_layer, in_axes=(0,))(x)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2: Custom class (registered as a PyTree) using the parameters of the Equinox Custom Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from jax.tree_util import tree_flatten, tree_unflatten, register_pytree_node_class\n",
    "\n",
    "@register_pytree_node_class\n",
    "@dataclass\n",
    "class PyTreeLinear:\n",
    "    weight: jnp.ndarray\n",
    "    bias: jnp.ndarray\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(self.weight, x) + self.bias\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        children = (self.weight, self.bias)\n",
    "        aux = None\n",
    "        return children, aux\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux, children):\n",
    "        return cls(*children)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      "\n",
      "out = \n",
      "[[ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      " [ 3.625029    0.02590603  3.410178    0.50355005]\n",
      " [ 0.8323103  -1.398576    0.28799635  0.3267359 ]\n",
      " [ 1.7365305  -0.21044779 -1.2441818  -0.9568258 ]\n",
      " [ 3.3418374   0.42115265  0.2816336  -1.6189749 ]\n",
      " [ 3.0669456  -0.31462416  0.10747854 -2.8793561 ]\n",
      " [ 1.9252988  -0.1918292  -1.0745429  -1.1775186 ]\n",
      " [ 1.645749   -0.4092752  -2.20948    -2.3588734 ]\n",
      " [ 2.2478125  -0.71958554  2.4841554   1.0517647 ]\n",
      " [ 3.4196887   0.1534388   0.6268668  -2.0282063 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer = PyTreeLinear(eqx_layer.weight, eqx_layer.bias)\n",
    "print(f'out = {layer(x[0])}\\n')\n",
    "print(f'out = \\n{jax.vmap(layer, in_axes=(0,))(x)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3: Applying a transformation on the parameters of the custom class between calls of the wrapper function\n",
    "##### bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_y = [ 3.804824    0.33519322  0.44742838 -2.7707675 ]\n",
      "log_softmax_pred_y = [-0.06517741 -3.5348084  -3.422573   -6.640769  ]\n",
      "y = 1\n",
      "loss = 3.534808397293091\n",
      "grads = \n",
      "(Array([[ 1.8816755e+00,  3.6422554e-01,  5.4732017e-02],\n",
      "       [-1.9498295e+00, -3.7741771e-01, -5.6714401e-02],\n",
      "       [ 6.5530933e-02,  1.2684460e-02,  1.9060884e-03],\n",
      "       [ 2.6230200e-03,  5.0772348e-04,  7.6295393e-05]], dtype=float32), Array([ 0.9369012 , -0.9708356 ,  0.03262837,  0.00130602], dtype=float32))\n",
      "\n",
      "SCALE DOWN\n",
      "\n",
      "pred_y = [-1.902412   -0.16759661 -0.22371419  1.3853837 ]\n",
      "log_softmax_pred_y = [-3.6586835  -1.9238681  -1.9799857  -0.37088773]\n",
      "y = 1\n",
      "loss = 1.9238680601119995\n",
      "grads = \n",
      "(Array([[ 5.1749345e-02,  1.0016835e-02,  1.5052255e-03],\n",
      "       [-1.7150941e+00, -3.3198130e-01, -4.9886689e-02],\n",
      "       [ 2.7730268e-01,  5.3675946e-02,  8.0658616e-03],\n",
      "       [ 1.3860421e+00,  2.6828852e-01,  4.0315602e-02]], dtype=float32), Array([ 0.02576641, -0.853959  ,  0.13807121,  0.6901214 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def loss_bs_1(params, aux, x, y):\n",
    "    model_ = PyTreeLinear.tree_unflatten(aux, params)\n",
    "    pred_y = model_(x)\n",
    "    jax.debug.print('pred_y = {pred_y}', pred_y=pred_y)\n",
    "    pred_y = jax.nn.log_softmax(pred_y)\n",
    "    jax.debug.print('log_softmax_pred_y = {pred_y}', pred_y=pred_y)\n",
    "    jax.debug.print('y = {y}', y=y)\n",
    "    loss_ = cross_entropy_bs_1(y, pred_y)\n",
    "    jax.debug.print('loss = {loss}', loss=loss_)\n",
    "    return loss_\n",
    "\n",
    "def cross_entropy_bs_1(y, pred_y):\n",
    "    # y is the true target: shape=(1,).\n",
    "    # pred_y ist the log-softmax'd prediction shape=(out_dim,).\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.array([y]), axis=0)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "params, aux = layer.tree_flatten()\n",
    "\n",
    "loss_, grads_ = jax.value_and_grad(loss_bs_1)(params, aux, x[0], y[0])\n",
    "print(f'grads = \\n{grads_}')\n",
    "\n",
    "print('\\nSCALE DOWN\\n')\n",
    "scale_fn = lambda x: x / (-2)\n",
    "layer_scaled_dn = jax.tree_map(scale_fn, layer)\n",
    "params_dn, aux_dn = layer_scaled_dn.tree_flatten()\n",
    "loss_, grads_ = jax.value_and_grad(loss_bs_1)(params_dn, aux_dn, x[0], y[0])\n",
    "print(f'grads = \\n{grads_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bs > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 3.1309831142425537\n",
      "grads = \n",
      "(Array([[ 0.66311485,  0.22464275,  0.22545396],\n",
      "       [-0.61407083, -0.10023337, -0.24351758],\n",
      "       [-0.02413169, -0.11227638,  0.01469187],\n",
      "       [-0.02491243, -0.01213299,  0.00337176]], dtype=float32), Array([ 0.7487441 , -0.945336  ,  0.1443587 ,  0.05223323], dtype=float32))\n",
      "\n",
      "SCALE DOWN\n",
      "\n",
      "grads = \n",
      "(Array([[ 0.01075298, -0.00505622,  0.01786483],\n",
      "       [-0.5837781 , -0.19591357, -0.21283932],\n",
      "       [ 0.13681674,  0.09540007,  0.09139632],\n",
      "       [ 0.4362084 ,  0.10556973,  0.10357817]], dtype=float32), Array([ 0.07103135, -0.72101754,  0.21408351,  0.43590268], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def loss(params, aux, x, y):\n",
    "    model_ = PyTreeLinear.tree_unflatten(aux, params)\n",
    "    pred_y = jax.vmap(model_, in_axes=(0,))(x)\n",
    "    pred_y = jax.nn.log_softmax(pred_y)\n",
    "    loss_ = cross_entropy(y, pred_y)\n",
    "    return loss_\n",
    "\n",
    "def cross_entropy(y, pred_y):\n",
    "    # y are the true targets: shape=(bs,).\n",
    "    # pred_y are the log-softmax'd prediction shape=(bs, out_dim).\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1) # we need to convert y to shape=(bs, 1) to be able to use it as an index\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "layer = PyTreeLinear(eqx_layer.weight, eqx_layer.bias)\n",
    "\n",
    "params, aux = layer.tree_flatten()\n",
    "loss_, grads_ = jax.value_and_grad(loss)(params, aux, x, y)\n",
    "print(f'loss = {loss_}')\n",
    "print(f'grads = \\n{grads_}')\n",
    "\n",
    "print('\\nSCALE DOWN\\n')\n",
    "scale_fn = lambda x: x / (-2)\n",
    "\n",
    "layer_scaled_dn = jax.tree_map(scale_fn, layer)\n",
    "params_dn, aux_dn = layer_scaled_dn.tree_flatten()\n",
    "loss_, grads_ = jax.value_and_grad(loss)(params_dn, aux_dn, x, y)\n",
    "print(f'grads = \\n{grads_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verifying the correctness of the gradients with Equinox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss = 3.1309831142425537\n",
      "grads.weight = \n",
      "[[ 0.66311485  0.22464275  0.22545396]\n",
      " [-0.61407083 -0.10023337 -0.24351758]\n",
      " [-0.02413169 -0.11227638  0.01469187]\n",
      " [-0.02491243 -0.01213299  0.00337176]]\n",
      "grads.bias = \n",
      "[ 0.7487441  -0.945336    0.1443587   0.05223323]\n"
     ]
    }
   ],
   "source": [
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad\n",
    "def loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    pred_y = jax.nn.log_softmax(pred_y)\n",
    "    loss_ = cross_entropy(y, pred_y)\n",
    "    return loss_\n",
    "\n",
    "loss_, grads_ = loss(eqx_layer, x, y)\n",
    "print(f'\\nloss = {loss_}')\n",
    "print(f'grads.weight = \\n{grads_.weight}')\n",
    "print(f'grads.bias = \\n{grads_.bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "module: https://github.com/patrick-kidger/equinox/blob/d7d2cb91dde3beee970d9f8f10fc3c9c7f2f0e39/equinox/_module.py#L969\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
