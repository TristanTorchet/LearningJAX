{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    t = jnp.linspace(0, 2 * math.pi, 16)\n",
    "    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = jrandom.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    steps=200,\n",
    "    hidden_size=16,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    data_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 2)\n",
    "    xs, ys = get_data(dataset_size, key=data_key)\n",
    "    iter_data = dataloader((xs, ys), batch_size)\n",
    "\n",
    "    model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def compute_loss(model, x, y):\n",
    "        pred_y = jax.vmap(model)(x)\n",
    "        # Trains with respect to binary cross-entropy\n",
    "        return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "    # Important for efficiency whenever you use JAX: wrap everything into a single JIT\n",
    "    # region.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, x, y, opt_state):\n",
    "        loss, grads = compute_loss(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    optim = optax.adam(learning_rate)\n",
    "    opt_state = optim.init(model)\n",
    "    print(f'{\"Step\":<6}|{\"Loss\":<10}')\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "        loss = loss.item()\n",
    "        print(f\"{step:<6}|{loss:<10.4f}\")\n",
    "\n",
    "    pred_ys = jax.vmap(model)(xs)\n",
    "    num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "    final_accuracy = (num_correct / dataset_size).item()\n",
    "    print(f\"final_accuracy={final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  |Loss      \n",
      "0     |0.7095    \n",
      "1     |0.6952    \n",
      "2     |0.6903    \n",
      "3     |0.6986    \n",
      "4     |0.6939    \n",
      "5     |0.6977    \n",
      "6     |0.6939    \n",
      "7     |0.6912    \n",
      "8     |0.6903    \n",
      "9     |0.6912    \n",
      "10    |0.6931    \n",
      "11    |0.6920    \n",
      "12    |0.6938    \n",
      "13    |0.6929    \n",
      "14    |0.6946    \n",
      "15    |0.6903    \n",
      "16    |0.6923    \n",
      "17    |0.6930    \n",
      "18    |0.6939    \n",
      "19    |0.6901    \n",
      "20    |0.6939    \n",
      "21    |0.6920    \n",
      "22    |0.6932    \n",
      "23    |0.6924    \n",
      "24    |0.6924    \n",
      "25    |0.6943    \n",
      "26    |0.6972    \n",
      "27    |0.6920    \n",
      "28    |0.6942    \n",
      "29    |0.6915    \n",
      "30    |0.6893    \n",
      "31    |0.6895    \n",
      "32    |0.6939    \n",
      "33    |0.6882    \n",
      "34    |0.6889    \n",
      "35    |0.7056    \n",
      "36    |0.6865    \n",
      "37    |0.6842    \n",
      "38    |0.6802    \n",
      "39    |0.6990    \n",
      "40    |0.6912    \n",
      "41    |0.7013    \n",
      "42    |0.6822    \n",
      "43    |0.6898    \n",
      "44    |0.6933    \n",
      "45    |0.7014    \n",
      "46    |0.6925    \n",
      "47    |0.6903    \n",
      "48    |0.6869    \n",
      "49    |0.6979    \n",
      "50    |0.6831    \n",
      "51    |0.7050    \n",
      "52    |0.6897    \n",
      "53    |0.6932    \n",
      "54    |0.7074    \n",
      "55    |0.7008    \n",
      "56    |0.6834    \n",
      "57    |0.6971    \n",
      "58    |0.6992    \n",
      "59    |0.6956    \n",
      "60    |0.6863    \n",
      "61    |0.6899    \n",
      "62    |0.6957    \n",
      "63    |0.6935    \n",
      "64    |0.6973    \n",
      "65    |0.6936    \n",
      "66    |0.6960    \n",
      "67    |0.6958    \n",
      "68    |0.6926    \n",
      "69    |0.6912    \n",
      "70    |0.6934    \n",
      "71    |0.6910    \n",
      "72    |0.6876    \n",
      "73    |0.6935    \n",
      "74    |0.6929    \n",
      "75    |0.6933    \n",
      "76    |0.6917    \n",
      "77    |0.6892    \n",
      "78    |0.6942    \n",
      "79    |0.6865    \n",
      "80    |0.6878    \n",
      "81    |0.7017    \n",
      "82    |0.6891    \n",
      "83    |0.6880    \n",
      "84    |0.6834    \n",
      "85    |0.6919    \n",
      "86    |0.6975    \n",
      "87    |0.6867    \n",
      "88    |0.6985    \n",
      "89    |0.6915    \n",
      "90    |0.6919    \n",
      "91    |0.7014    \n",
      "92    |0.6838    \n",
      "93    |0.7120    \n",
      "94    |0.6827    \n",
      "95    |0.6926    \n",
      "96    |0.6962    \n",
      "97    |0.6925    \n",
      "98    |0.7001    \n",
      "99    |0.6874    \n",
      "100   |0.6909    \n",
      "101   |0.6879    \n",
      "102   |0.6924    \n",
      "103   |0.6900    \n",
      "104   |0.6910    \n",
      "105   |0.6905    \n",
      "106   |0.6902    \n",
      "107   |0.6865    \n",
      "108   |0.6877    \n",
      "109   |0.6864    \n",
      "110   |0.6885    \n",
      "111   |0.6820    \n",
      "112   |0.6915    \n",
      "113   |0.6860    \n",
      "114   |0.6858    \n",
      "115   |0.6913    \n",
      "116   |0.6755    \n",
      "117   |0.6772    \n",
      "118   |0.6753    \n",
      "119   |0.6930    \n",
      "120   |0.6794    \n",
      "121   |0.6735    \n",
      "122   |0.6818    \n",
      "123   |0.6742    \n",
      "124   |0.6689    \n",
      "125   |0.6551    \n",
      "126   |0.6510    \n",
      "127   |0.6452    \n",
      "128   |0.6318    \n",
      "129   |0.6112    \n",
      "130   |0.6008    \n",
      "131   |0.5948    \n",
      "132   |0.5723    \n",
      "133   |0.5384    \n",
      "134   |0.5097    \n",
      "135   |0.4864    \n",
      "136   |0.5476    \n",
      "137   |0.5420    \n",
      "138   |0.3764    \n",
      "139   |0.3550    \n",
      "140   |0.3104    \n",
      "141   |0.2933    \n",
      "142   |0.2297    \n",
      "143   |0.2111    \n",
      "144   |0.1863    \n",
      "145   |0.1470    \n",
      "146   |0.1335    \n",
      "147   |0.1081    \n",
      "148   |0.0864    \n",
      "149   |0.0763    \n",
      "150   |0.0642    \n",
      "151   |0.0595    \n",
      "152   |0.0455    \n",
      "153   |0.0405    \n",
      "154   |0.0356    \n",
      "155   |0.0310    \n",
      "156   |0.0277    \n",
      "157   |0.0253    \n",
      "158   |0.0224    \n",
      "159   |0.0205    \n",
      "160   |0.0187    \n",
      "161   |0.0170    \n",
      "162   |0.0152    \n",
      "163   |0.0147    \n",
      "164   |0.0134    \n",
      "165   |0.0123    \n",
      "166   |0.0115    \n",
      "167   |0.0107    \n",
      "168   |0.0101    \n",
      "169   |0.0096    \n",
      "170   |0.0092    \n",
      "171   |0.0088    \n",
      "172   |0.0084    \n",
      "173   |0.0077    \n",
      "174   |0.0076    \n",
      "175   |0.0072    \n",
      "176   |0.0069    \n",
      "177   |0.0068    \n",
      "178   |0.0064    \n",
      "179   |0.0063    \n",
      "180   |0.0060    \n",
      "181   |0.0058    \n",
      "182   |0.0057    \n",
      "183   |0.0055    \n",
      "184   |0.0054    \n",
      "185   |0.0053    \n",
      "186   |0.0052    \n",
      "187   |0.0051    \n",
      "188   |0.0049    \n",
      "189   |0.0048    \n",
      "190   |0.0047    \n",
      "191   |0.0047    \n",
      "192   |0.0046    \n",
      "193   |0.0044    \n",
      "194   |0.0044    \n",
      "195   |0.0043    \n",
      "196   |0.0043    \n",
      "197   |0.0042    \n",
      "198   |0.0042    \n",
      "199   |0.0041    \n",
      "final_accuracy=1.0000\n"
     ]
    }
   ],
   "source": [
    "main()  # All right, let's run the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "key = jrandom.PRNGKey(0)\n",
    "data_key, model_key = jrandom.split(key, 2)\n",
    "model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  hidden_size=16,\n",
      "  cell=GRUCell(\n",
      "    weight_ih=f32[48,2],\n",
      "    weight_hh=f32[48,16],\n",
      "    bias=f32[48],\n",
      "    bias_n=f32[16],\n",
      "    input_size=2,\n",
      "    hidden_size=16,\n",
      "    use_bias=True\n",
      "  ),\n",
      "  linear=Linear(\n",
      "    weight=f32[1,16],\n",
      "    bias=None,\n",
      "    in_features=16,\n",
      "    out_features=1,\n",
      "    use_bias=False\n",
      "  ),\n",
      "  bias=f32[1]\n",
      ")\n",
      "(48, 16)\n",
      "(48, 2)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.cell.weight_hh.shape)\n",
    "print(model.cell.weight_ih.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    @jax.named_scope(\"eqx.nn.GRUCell\")\n",
    "    def __call__(\n",
    "        self, input: Array, hidden: Array, *, key: Optional[PRNGKeyArray] = None\n",
    "    ):\n",
    "        \"\"\"**Arguments:**\n",
    "\n",
    "        - `input`: The input, which should be a JAX array of shape `(input_size,)`.\n",
    "        - `hidden`: The hidden state, which should be a JAX array of shape\n",
    "            `(hidden_size,)`.\n",
    "        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n",
    "            (Keyword only argument.)\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        The updated hidden state, which is a JAX array of shape `(hidden_size,)`.\n",
    "        \"\"\"\n",
    "        if self.use_bias:\n",
    "            bias = self.bias\n",
    "            bias_n = self.bias_n\n",
    "        else:\n",
    "            bias = 0\n",
    "            bias_n = 0\n",
    "        igates = jnp.split(self.weight_ih @ input + bias, 3) # [Wzx @ x, Wrx @ x, Wnx @ x]\n",
    "        hgates = jnp.split(self.weight_hh @ hidden, 3) # [Wzh @ h, Wrh @ h, Wn @ h]\n",
    "        reset = jnn.sigmoid(igates[0] + hgates[0]) # r = σ(Wzx @ x + Wzh @ h)\n",
    "        inp = jnn.sigmoid(igates[1] + hgates[1]) # z = σ(Wrx @ x + Wrh @ h)\n",
    "        new = jnn.tanh(igates[2] + reset * (hgates[2] + bias_n)) # n = tanh(Wnx @ x + r * (Wn @ h))\n",
    "        return new + inp * (hidden - new)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
